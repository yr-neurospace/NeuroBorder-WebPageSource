{
  "hash": "98719406d645313c519c13b8bc454767",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"MPRA analysis pipeline\"\nauthor: \"Rui Yang\"\ndate: \"2025-01-08\"\ndate-modified: last-modified\ncategories: [mpra]\nformat:\n  html:\n    toc: true\n    toc-depth: 6\n    toc-location: left\n    fig-align: center\n    number-depth: 6\n    number-sections: true\n    fig-cap-location: bottom\n    fig-format: png\n    lightbox: true\n    tbl-cap-location: top\n    page-layout: full\njupyter: julia-1.10\nexecute:\n  warning: false\n  eval: false\n---\n\n\n\n\n## Link barcodes to CREs\n\nBefore running any of the following steps, you should rename your FASTQ files according to [these rules](https://www.neuroborder.com/Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/#copy-your-raw-fastq-data-to-a-new-directory-and-rename-them).\n\n::: {#1ad44b0e .cell execution_count=1}\n``` {.julia .cell-code}\nwork_dir = \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20250430\"\n\ncd(work_dir)\n```\n:::\n\n\n```{r}\nwork_dir <- \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20250430\"\n\nsetwd(work_dir)\n```\n\n\n\n\n### MD5SUM check over raw FASTQ files\n\n::: {#9cf08585 .cell execution_count=2}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n```\n:::\n\n\n### FASTQC over raw FASTQ files\n\n::: {#9b787e89 .cell execution_count=3}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n```\n:::\n\n\n### Quality trimming over raw FASTQ files\n\n::: {#b962e5fd .cell execution_count=4}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)\n```\n:::\n\n\n### FASTQC over clean FASTQ files\n\n::: {#eeb28ede .cell execution_count=5}\n``` {.julia .cell-code}\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n```\n:::\n\n\n### Generate reference sequences\n\nWrite each reference sequence containing CRE as well as other necessary sequences and its ID into a FASTA file, which will be used later to build Bowtie2 reference index.\n\n::: {#6f58342d .cell execution_count=6}\n``` {.julia .cell-code}\nusing CSV, DataFrames\n\nleft_seq = \"TTCTCTGGCCTAACTGTCTAGACCTGCAGGAGGACCGGATCAACT\"\nright_seq = \"CATTGCGTGAACCGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTGCAAAGTGAACACATCGCTAAGCGAAAGCTAAGNNNNNNNNNNNNNNNACCGGTCGCCACCATGGTGAGCAAGG\"\nmpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.tsv\"\nmpra_ctl_file = \"ref/null_sequences.tsv\"\noutput_mpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.dealed.tsv\"\noutput_mpra_ctl_file = \"ref/null_sequences.dealed.tsv\"\noutput_ref_fa_file = \"ref/mpra_ref.fa\"\n\nmpra_test = CSV.read(mpra_test_file, DataFrame)\nmpra_test = unique(mpra_test)\nmpra_test = transform(mpra_test, \"PSCE\", \"extended_mm10_seq\" => (x -> string.(left_seq, x, right_seq)) => \"attached_seq\")\nmpra_test = groupby(mpra_test, \"PSCE\")\nmpra_test = transform(mpra_test, nrow => \"num_per_PSCE\", eachindex => \"PSCE_sub_rank\")\nmpra_test = transform(mpra_test, [\"PSCE\", \"PSCE_sub_rank\", \"num_per_PSCE\"] => ByRow((x, y, z) -> begin\n    if z == 1\n        x\n    else\n        string.(x, \"_\", y)\n    end\nend) => \"PSCE_new_id\")\n\nCSV.write(output_mpra_test_file, mpra_test; delim=\"\\t\", append=false)\n\nmpra_ctl = CSV.read(mpra_ctl_file, DataFrame)\nmpra_ctl = unique(mpra_ctl)\nmpra_ctl = transform(mpra_ctl, eachindex => \"rank\")\nmpra_ctl = transform(mpra_ctl, \"rank\" => (x -> string.(\"CTL\", x)) => \"PSCE\", \"seq\" => (x -> string.(left_seq, x, right_seq)) => \"attached_seq\")\n\nCSV.write(output_mpra_ctl_file, mpra_ctl; delim=\"\\t\", append=false)\n\nref_fa = vcat(string.(\">\", mpra_test[!, \"PSCE_new_id\"], \"\\n\", mpra_test[!, \"attached_seq\"]),\n    string.(\">\", mpra_ctl[!, \"PSCE\"], \"\\n\", mpra_ctl[!, \"attached_seq\"]))\n\nopen(output_ref_fa_file, \"w\") do io\n    for line in ref_fa\n        println(io, line)\n    end\nend\n```\n:::\n\n\n### Build Bowtie2 index\n\n::: {#0d5f4949 .cell execution_count=7}\n``` {.julia .cell-code}\nusing YRUtils\n\nref_fa = \"ref/mpra_ref.fa\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mpra_ref\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" => \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend\n```\n:::\n\n\n### Align reads with Bowtie2\n\n::: {#261cb14a .cell execution_count=8}\n``` {.julia .cell-code}\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mpra_ref\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) > 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c > \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) > 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c > \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 --np 0 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -> println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n```\n:::\n\n\n### Remove reads unmapped and with low quality\n\n::: {#032fd35f .cell execution_count=9}\n``` {.julia .cell-code}\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend\n```\n:::\n\n\n### Extract CRE-Barcode pairs\n\n::: {#f049cafb .cell execution_count=10}\n``` {.julia .cell-code}\nusing XAM, FASTX, CSV, DataFrames, YRUtils, Serialization\n\nfunction extract_cre_bc_pairs(bam_file::AbstractString, ref_dict::Dict{String,String};\n    barcode_length::Int=15, quality_scheme::Int=33)\n    valid_dna_bases = (\"A\", \"T\", \"C\", \"G\")\n\n    total_num_records = 0\n    xn_num_records = 0\n    complete_xn_num_records = 0\n    valid_xn_num_records = 0\n    typical_aln_vec_dict = Dict{String,Vector{String}}()\n\n    cre_bc_vec = Tuple{String,String}[]\n    reader = open(BAM.Reader, bam_file)\n    record = BAM.Record()\n    while !eof(reader)\n        empty!(record)\n        read!(reader, record)\n        total_num_records += 1\n        # The optional field XN:i:<N> reports the number of ambiguous reference characters (e.g. N) overlapped by an alignment\n        if haskey(record, \"XN\") && record[\"XN\"] == barcode_length\n            xn_num_records += 1\n            ref_name = BAM.refname(record)\n            # The leftmost mapping position\n            # BAM is 0-based, while SAM is 1-based\n            # BAM.position() gets the 1-based leftmost mapping position of record\n            ref_pos = BAM.position(record)\n            ref_seq = ref_dict[ref_name]\n            cigar_str = BAM.cigar(record)\n            query_seq = string(BAM.sequence(record))\n            query_qual_char_seq = join(Char.(BAM.quality(record) .+ quality_scheme))\n\n            aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_seq, ref_pos; truncate_ref=false))\n            qual_aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_qual_char_seq, ref_pos; truncate_ref=false))\n\n            ref_m = match(Regex(string(\"N{\", barcode_length, \"}\")), aln_vec[1])\n            if !isnothing(ref_m)\n                complete_xn_num_records += 1\n                extract_range = ref_m.offset:(ref_m.offset+barcode_length-1)\n                barcode_seq = aln_vec[2][extract_range]\n                barcode_qual_char_seq = qual_aln_vec[2][extract_range]\n                if all(split(barcode_seq, \"\") .∈ Ref(valid_dna_bases)) && all([Int(c) - quality_scheme for c in barcode_qual_char_seq] .>= base_qual)\n                    valid_xn_num_records += 1\n                    push!(cre_bc_vec, (ref_name, barcode_seq))\n                    typical_aln_vec_dict[string(ref_name, \":\", barcode_seq)] = aln_vec\n                end\n            end\n        end\n    end\n    close(reader)\n\n    cre_bc_gdf = groupby(DataFrame(cre_bc_vec, [:cre, :barcode]), [:cre, :barcode])\n    uniq_cre_bc_df = sort(combine(cre_bc_gdf, nrow => \"num\", proprow => \"prop\"), :num, rev=true)\n\n    open(replace(bam_file, r\"\\.bam$\" => \".extract_cre_bc_pairs.log\"), \"w\") do io\n        println(io, string(\n            \"The number of records in total: \", total_num_records, \"\\n\",\n            \"The number of records with XN field: \", xn_num_records, \"\\n\",\n            \"The number of records with complete barcode: \", complete_xn_num_records, \"\\n\",\n            \"The number of records passing base and quality check: \", valid_xn_num_records, \"\\n\",\n            \"The number of records non-redundant: \", nrow(uniq_cre_bc_df)\n        ))\n    end\n\n    return [uniq_cre_bc_df, typical_aln_vec_dict]\nend\n\nref_file = \"ref/mpra_ref.fa\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nbase_qual = 20\n\n# Read in reference sequences\nref_dict = FASTAReader(open(ref_file, \"r\")) do reader\n    dict = Dict{String,String}()\n    for record in reader\n        dict[identifier(record)] = sequence(record)\n    end\n    return dict\nend\n\nbam_files = YRUtils.BaseUtils.list_files(high_qual_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    # Extract CRE-Barcode pairs\n    cre_bc_res = extract_cre_bc_pairs(bam_file, ref_dict)\n\n    CSV.write(replace(bam_file, r\"\\.bam$\" => \".uniq_cre_bc_pairs.tsv\"),\n        cre_bc_res[1]; delim=\"\\t\", append=false, writeheader=true)\n\n    # obj = open(jls_file, \"r\") do io\n    #     deserialize(io)\n    # end\n    open(replace(bam_file, r\"\\.bam$\" => \".typical_cre_bc_aligned_sequences.jls\"), \"w\") do io\n        serialize(io, cre_bc_res[2])\n    end\n\n    rand_keys = rand(keys(cre_bc_res[2]), 100)\n    rand_dict = Dict(k => cre_bc_res[2][k] for k in rand_keys)\n    YRUtils.BioUtils.show_align(rand_dict,\n        replace(bam_file, r\"\\.bam$\" => \".typical_cre_bc_aligned_sequences.100.html\");\n        wrap_width=120)\nend\n```\n:::\n\n\n### Quality check\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(ggprism)\n\ninput_dir <- \"high_qual_bam\"\n\ncre_bc_files <- list.files(input_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\nfor (cre_bc_file in cre_bc_files) {\n    cre_bc_df <- vroom(cre_bc_file) %>%\n        select(cre, barcode) %>%\n        distinct()\n\n    cre_count_df <- count(cre_bc_df, cre) %>%\n        mutate(type = if_else(str_detect(cre, \"^CTL\"), \"CTL\", \"CRE\")) %>%\n        rename(cre_bc = cre)\n    bc_count_df <- count(cre_bc_df, barcode) %>%\n        mutate(type = \"BC\") %>%\n        rename(cre_bc = barcode)\n    count_df <- bind_rows(cre_count_df, bc_count_df)\n\n    cre_quantiles <- quantile(count_df$n[count_df$type %in% c(\"CTL\", \"CRE\")], probs = seq(0, 1, 0.1))\n    cre_only_quantiles <- quantile(count_df$n[count_df$type == \"CRE\"], probs = seq(0, 1, 0.1))\n    ctl_only_quantiles <- quantile(count_df$n[count_df$type == \"CTL\"], probs = seq(0, 1, 0.1))\n    bc_quantiles <- quantile(count_df$n[count_df$type == \"BC\"], probs = seq(0, 1, 0.1))\n    type_nums <- table(count_df$type)\n\n    paste0(\n        \"1. CRE/CTL quantiles (\", type_nums[\"CTL\"] + type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_quantiles), \"\\t\", cre_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"2. CRE only quantiles (\", type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_only_quantiles), \"\\t\", cre_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"3. CTL only quantiles (\", type_nums[\"CTL\"], \"): \\n\",\n        paste0(paste0(names(ctl_only_quantiles), \"\\t\", ctl_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"4. BC quantiles (\", type_nums[\"BC\"], \"): \\n\",\n        paste0(paste0(names(bc_quantiles), \"\\t\", bc_quantiles), collapse = \"\\n\")\n    ) %>% vroom_write_lines(file = gsub(\"\\\\.tsv$\", \"\\\\.quantiles.txt\", cre_bc_file))\n\n    p <- ggplot(count_df, aes(type, log2(n), fill = type, color = type)) +\n        geom_violin(scale = \"width\", alpha = 0.25, trim = TRUE) +\n        geom_boxplot(width = 0.2, outliers = FALSE, alpha = 0.25) +\n        scale_y_continuous(expand = expansion(mult = c(0.05, 0))) +\n        labs(\n            x = \"Sequence Type\",\n            y = \"log2(Count)\"\n        ) +\n        theme_prism(base_size = 20, base_family = \"Arial\", border = FALSE) +\n        theme(legend.position = \"none\")\n    ppreview(p, file = gsub(\"\\\\.tsv$\", \"\\\\.violin.pdf\", cre_bc_file))\n}\n```\n\n\n\n\n### Merge all CRE-Barcode pairs\n\n\n\n\n```{r}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(vroom)\nlibrary(YRUtils)\n\n# accept only one parameter (i.e., sample file names)\n# and return the sample IDs derived from the sample file names\nextract_sample_id <- function(x) {\n    gsub(\n        \"^/data/users/dell/mpra/link_barcode_to_cre/|high_qual_bam/|\\\\.chr_srt\\\\.uniq_cre_bc_pairs\\\\.tsv$\",\n        \"\", file\n    )\n}\n\ninput_dirs <- c(\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20231027/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/pcr_v20230922/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20250430/high_qual_bam\"\n)\noutput_dir <- \"/data/users/dell/mpra/link_barcode_to_cre/final_result\"\nstats_columns <- c(\"id\", \"cre\", \"barcode\")\n\nfiles <- list.files(input_dirs, pattern = \"\\\\.tsv$\", recursive = FALSE, full.names = TRUE)\ncre_bc_df <- tibble()\nfor (file in files) {\n    cre_bc_df <- bind_rows(\n        cre_bc_df,\n        vroom(file) %>%\n            select(cre, barcode) %>%\n            mutate(\n                id = paste0(cre, \":\", barcode),\n                sample = extract_sample_id(file)\n            )\n    )\n}\n\n# check overlapping ratios across samples using unique CRE-Barcode pairs/CREs/barcodes\nlayout <- \"\n#A\nBC\n\"\nfor (stats_column in stats_columns) {\n    ls <- lapply(split(cre_bc_df, cre_bc_df$sample), function(df) {\n        unique(na.omit(df[[stats_column]]))\n    })\n\n    ls_copy <- ls\n    df <- data.frame()\n    for (k in length(ls_copy):2) {\n        name_combn_vec <- combn(names(ls_copy), k) |>\n            apply(2, function(name_combn) {\n                paste0(name_combn, collapse = \"<*:*>\")\n            })\n\n        intersect_num_vec <- c()\n        for (name_combn in name_combn_vec) {\n            name_vec <- strsplit(name_combn, \"<*:*>\", fixed = TRUE)[[1]]\n            intersect_items <- Reduce(intersect, ls_copy[name_vec])\n            intersect_num_vec <- c(intersect_num_vec, length(intersect_items))\n            for (name in name_vec) {\n                ls_copy[[name]] <- ls_copy[[name]][!(ls_copy[[name]] %in% intersect_items)]\n            }\n        }\n\n        df <- rbind(\n            df,\n            data.frame(\n                combn_k = k,\n                combn = name_combn_vec,\n                intersect_num = intersect_num_vec\n            )\n        )\n    }\n    df <- rbind(\n        df,\n        data.frame(\n            combn_k = 1,\n            combn = names(ls_copy),\n            intersect_num = sapply(ls_copy, length)\n        )\n    )\n    row.names(df) <- NULL\n\n    for (name in names(ls)) {\n        df[[name]] <- grepl(name, df$combn, fixed = TRUE)\n    }\n\n    df <- pivot_longer(\n        df,\n        cols = -all_of(c(\"combn_k\", \"combn\", \"intersect_num\")),\n        names_to = \"sample\", values_to = \"is_match\"\n    ) |>\n        filter((intersect_num > 0) & is_match)\n    df <- df |>\n        arrange(combn_k, desc(intersect_num)) |>\n        mutate(\n            sample = factor(sample, levels = rev(unique(sample))),\n            combn = factor(combn, levels = unique(combn))\n        )\n\n    set_size_df <- data.frame(\n        sample = names(ls),\n        num = sapply(ls, length)\n    ) |>\n        mutate(sample = factor(sample, levels = levels(df$sample)))\n    row.names(set_size_df) <- NULL\n\n    mlog10 <- scales::new_transform(\n        name = \"mlog10\",\n        transform = function(x) -log10(x),\n        inverse = function(x) 10^-x,\n        breaks = scales::breaks_log(),\n        format = scales::label_scientific(),\n        domain = c(.Machine$double.xmin, .Machine$double.xmax)\n    )\n\n    p_set_size_bar <- ggplot(set_size_df, aes(num, sample)) +\n        geom_bar(stat = \"identity\") +\n        scale_y_discrete(position = \"right\") +\n        scale_x_continuous(\n            expand = expansion(0),\n            transform = mlog10\n        ) +\n        labs(x = NULL, y = NULL) +\n        theme_minimal() +\n        theme(\n            axis.text.y = element_blank(),\n            axis.ticks.y = element_blank(),\n            axis.ticks.x = element_line()\n        )\n\n    p_set_intersection_point <- ggplot(df, aes(combn, sample)) +\n        geom_point() +\n        geom_line(aes(group = combn)) +\n        scale_y_discrete(position = \"right\") +\n        labs(x = NULL, y = NULL) +\n        theme_bw() +\n        theme(\n            axis.text.x = element_blank(),\n            axis.ticks.x = element_blank()\n        )\n\n    p_set_intersection_bar <- ggplot(\n        distinct(select(df, combn, intersect_num)),\n        aes(combn, intersect_num)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            transform = \"log10\",\n            position = \"right\",\n            labels = scales::label_scientific()\n        ) +\n        labs(x = NULL, y = NULL) +\n        theme_bw() +\n        theme(\n            axis.text.x = element_blank(),\n            axis.ticks.x = element_blank()\n        )\n\n    p <- p_set_intersection_bar + p_set_size_bar + p_set_intersection_point +\n        plot_layout(\n            design = layout,\n            widths = c(1, 3),\n            heights = c(2, 1)\n        )\n    ppreview(p, file = file.path(output_dir, paste0(stats_column, \"_overlapping_ratios.upset.pdf\")))\n\n    stats_df <- df |>\n        select(combn_k, combn, intersect_num) |>\n        distinct() |>\n        reframe(\n            combn_k = combn_k,\n            combn = combn,\n            intersect_num = intersect_num,\n            intersect_num_percent = intersect_num / sum(intersect_num) * 100\n        ) |>\n        arrange(combn_k, intersect_num)\n    vroom_write(stats_df, file = file.path(output_dir, paste0(stats_column, \"_overlapping_ratios.stats.txt\")), col_names = TRUE, append = FALSE)\n}\n\n# output merged CRE-Barcode pairs\nmerged_cre_bc_df <- cre_bc_df %>%\n    select(cre, barcode) %>%\n    distinct()\none_cre_barocdes <- merged_cre_bc_df %>%\n    group_by(barcode) %>%\n    count() %>%\n    filter(n == 1) %>%\n    pull(barcode) %>%\n    unique()\none_cre_bc_df <- merged_cre_bc_df %>%\n    filter(barcode %in% one_cre_barocdes) %>%\n    distinct()\n\nvroom_write(merged_cre_bc_df, file = file.path(output_dir, \"redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)\nvroom_write(one_cre_bc_df, file = file.path(output_dir, \"non_redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)\n```\n\n\n\n\n## Count barcodes\n\nBefore running any of the following steps, you should rename your FASTQ files in this form: `ID_(RNA|DNA)_repN[_partN].R[123].(fq|fastq).gz` (`ID` can only contain `[a-zA-Z0-9]`; `N` can only contain `[0-9]`).\n\n::: {#060da5df .cell execution_count=11}\n``` {.julia .cell-code}\nwork_dir = \"/data/users/dell/mpra/count_barcode/15bp_v20240627\"\n\ncd(work_dir)\n```\n:::\n\n\n```{r}\nwork_dir <- \"/data/users/dell/mpra/count_barcode/15bp_v20240627\"\n\nsetwd(work_dir)\n```\n\n\n\n\n### MD5SUM check over raw FASTQ files\n\n::: {#6a330950 .cell execution_count=12}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n```\n:::\n\n\n### FASTQC over raw FASTQ files\n\n::: {#f2c87a29 .cell execution_count=13}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n```\n:::\n\n\n### Count barcodes\n\n#### Solution 1\n\n::: {#47a0a501 .cell execution_count=14}\n``` {.julia .cell-code}\nusing FASTX, DataFrames, CSV, YRUtils, CodecZlib\n\nraw_fastq_dir = \"raw_fastq\"\ncre_bc_file = \"/data/users/dell/mpra/link_barcode_to_cre/final_result/redundant_cre_bc_pairs.tsv\"\nraw_bc_umi_dir = \"raw_bc_umi\"\nbase_qual = 20\nseqkit_nthreads = 40\n\nmkpath(raw_bc_umi_dir)\ncre_bc_df = CSV.read(cre_bc_file, DataFrame; header=true, delim=\"\\t\")\nuniq_barcodes = Set(cre_bc_df[:, :barcode])\n\nraw_fastq_file_name_pattern = r\".+/(?<id>[a-zA-Z0-9]+)_(?<type>RNA|DNA)_(?<rep>rep[0-9]+)(_(?<tech>part[0-9]+))?\\.(?<read>R[123])\\.(fq|fastq)\\.gz$\"\nraw_fastq_files = YRUtils.BioUtils.list_files(raw_fastq_dir, raw_fastq_file_name_pattern; recursive=false, full_name=true)\nraw_fastq_nums = YRUtils.BioUtils.fq_num(raw_fastq_files, seqkit_nthreads)\nms = match.(raw_fastq_file_name_pattern, raw_fastq_files)\nmetadata_vec = Vector{NTuple{7,String}}(undef, length(ms))\nfor i in 1:length(ms)\n    metadata_vec[i] = (ms[i].match, ms[i][\"id\"], ms[i][\"type\"], ms[i][\"rep\"], ms[i][\"tech\"], ms[i][\"read\"], string(raw_fastq_nums[ms[i].match]))\nend\ndf = unique(DataFrame(metadata_vec, [:file, :id, :type, :rep, :tech, :read, :num_seqs]))\ndf = transform(\n    df,\n    [:id, :type, :rep, :tech] => ByRow((id, type, rep, tech) -> join([id, type, rep, tech], \"_\")) => :tech_sample,\n    [:id, :type, :rep] => ByRow((id, type, rep) -> join([id, type, rep], \"_\")) => :rep_sample,\n    [:id, :type] => ByRow((id, type) -> join([id, type], \"_\")) => :type_sample\n)\ntech_gdf = groupby(df, :tech_sample)\nCSV.write(joinpath(raw_bc_umi_dir, \"fq_metadata.tsv\"), df; header=true, delim=\"\\t\", append=false)\n\ndf_dict = Dict(unique(read_df[:, :tech_sample])[1] => DataFrame() for read_df in tech_gdf)\nfor read_df in tech_gdf\n    # Read in reads\n    read_dict = Dict(read_type => Vector{Tuple{String,String,String}}(undef, parse(Int64, num_seqs)) for (read_type, num_seqs) in collect(zip(read_df[:, :read], read_df[:, :num_seqs])))\n    Threads.@threads for (read_type, num_seqs, fq_file) in collect(zip(read_df[:, :read], read_df[:, :num_seqs], read_df[:, :file]))\n        @info string(\"start parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" ...\")\n        FASTQReader(GzipDecompressorStream(open(fq_file))) do reader\n            record = FASTQ.Record()\n            i = 0\n            while !eof(reader)\n                i += 1\n                empty!(record)\n                read!(reader, record)\n                read_dict[read_type][i] = (identifier(record), sequence(record), join(collect(quality_scores(record)), \"/\"))\n            end\n            @info string(\"read in \", i, \" sequences in total for \", fq_file)\n            if i != parse(Int64, num_seqs)\n                @error string(\"parsing file \", fq_file, \" failed!\")\n            end\n        end\n        @info string(\"parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" done!\")\n    end\n\n    # Count qualified barcodes and their UMIs\n    len_vec = length.(values(read_dict))\n    # The three files should have the same number of lines\n    if length(unique(len_vec)) == 1\n        bc_umi_vec = Vector{Tuple{String,String,String,Vararg{Bool,9}}}(undef, len_vec[1])\n        Threads.@threads for i in 1:len_vec[1]\n            # Read IDs should be identical across R1, R2, and R3\n            if read_dict[\"R1\"][i][1] == read_dict[\"R2\"][i][1] == read_dict[\"R3\"][i][1]\n                bc_umi_vec[i] = (\n                    # Read 1\n                    read_dict[\"R1\"][i][2],\n                    # Read 2\n                    read_dict[\"R2\"][i][2],\n                    # UMI\n                    read_dict[\"R3\"][i][2],\n                    # Read sequences should only contain A, T, C, and G across R1, R2, and R3\n                    !occursin(\"N\", string(read_dict[\"R1\"][i][2], read_dict[\"R2\"][i][2], read_dict[\"R3\"][i][2])),\n                    # All base qualities >= base_qual across R1, R2, and R3\n                    all(parse.(Int, split(string(read_dict[\"R1\"][i][3], \"/\", read_dict[\"R2\"][i][3], \"/\", read_dict[\"R3\"][i][3]), \"/\")) .>= base_qual),\n                    # Read 1 and read 2 should be reverse and complementary\n                    YRUtils.BioUtils.rev_com_dna_seq(read_dict[\"R1\"][i][2]) == read_dict[\"R2\"][i][2],\n                    # Either read 1 or read 2 should be in the barcode library (not both in theory)\n                    # Read 1 in the barcode library?\n                    read_dict[\"R1\"][i][2] in uniq_barcodes,\n                    # Read 2 in the barcode library?\n                    read_dict[\"R2\"][i][2] in uniq_barcodes,\n                    # The reverse sequence of read 1 in the barcode library?\n                    YRUtils.BioUtils.rev_seq(read_dict[\"R1\"][i][2]) in uniq_barcodes,\n                    # The complementary sequence of read 1 in the barcode library?\n                    YRUtils.BioUtils.com_dna_seq(read_dict[\"R1\"][i][2]) in uniq_barcodes,\n                    # The reverse sequence of read 2 in the barcode library?\n                    YRUtils.BioUtils.rev_seq(read_dict[\"R2\"][i][2]) in uniq_barcodes,\n                    # The complementary sequence of read 2 in the barcode library?\n                    YRUtils.BioUtils.com_dna_seq(read_dict[\"R2\"][i][2]) in uniq_barcodes\n                )\n            else\n                @error \"read IDs are not identical across R1, R2, and R3\"\n            end\n        end\n    else\n        @error \"length(R1) == length(R2) == length(R3) is not true\"\n    end\n\n    # Write statistics\n    col4, col5, col6, col7, col8, col9, col10, col11, col12 = (\n        getindex.(bc_umi_vec, 4),\n        getindex.(bc_umi_vec, 5),\n        getindex.(bc_umi_vec, 6),\n        getindex.(bc_umi_vec, 7),\n        getindex.(bc_umi_vec, 8),\n        getindex.(bc_umi_vec, 9),\n        getindex.(bc_umi_vec, 10),\n        getindex.(bc_umi_vec, 11),\n        getindex.(bc_umi_vec, 12)\n    )\n    open(joinpath(raw_bc_umi_dir, \"fq_read_stats.txt\"), \"a\") do io\n        stat_str = string(\n            \"==> \", unique(read_df[:, :tech_sample])[1], \" <==\\n\",\n            \"1. The number of reads in total: \", len_vec[1], \"\\n\\n\",\n            \"2. % reads without Ns (\", sum(col4), \"): \", sum(col4) / len_vec[1], \"\\n\",\n            \"3. % reads with base qualities >= \", base_qual, \" (\", sum(col5), \"): \", sum(col5) / len_vec[1], \"\\n\",\n            \"4. % reads passing 2 and 3: (\", sum(col4 .&& col5), \"): \", sum(col4 .&& col5) / len_vec[1], \"\\n\\n\",\n            \"5. % reads (R1 and R2 are reverse and complementary) (\", sum(col6), \"): \", sum(col6) / len_vec[1], \"\\n\",\n            \"6. % reads passing 2, 3, and 5 (\", sum(col4 .&& col5 .&& col6), \"): \", sum(col4 .&& col5 .&& col6) / len_vec[1], \"\\n\\n\",\n            \"7. % reads of R1 in the library (\", sum(col7), \"): \", sum(col7) / len_vec[1], \"\\n\",\n            \"8. % reads passing 2, 3, 5 and 7 (\", sum(col4 .&& col5 .&& col6 .&& col7), \"): \", sum(col4 .&& col5 .&& col6 .&& col7) / len_vec[1], \"\\n\\n\",\n            \"9. % reads of R2 in the library (\", sum(col8), \"): \", sum(col8) / len_vec[1], \"\\n\",\n            \"10. % reads passing 2, 3, 5 and 9 (\", sum(col4 .&& col5 .&& col6 .&& col8), \"): \", sum(col4 .&& col5 .&& col6 .&& col8) / len_vec[1], \"\\n\\n\",\n            \"11. % reads (both R1 and R2 are in the library) (\", sum(col7 .&& col8), \"): \", sum(col7 .&& col8) / len_vec[1], \"\\n\",\n            \"12. % reads passing 2, 3, 5 and 11 (\", sum(col4 .&& col5 .&& col6 .&& col7 .&& col8), \"): \", sum(col4 .&& col5 .&& col6 .&& col7 .&& col8) / len_vec[1], \"\\n\\n\",\n            \"13. % reads (one of R1 and R2 in the library, not both) (\", sum(col7 .⊻ col8), \"): \", sum(col7 .⊻ col8) / len_vec[1], \"\\n\",\n            \"14. % reads passing 2, 3, 5 and 13 (\", sum(col4 .&& col5 .&& col6 .&& (col7 .⊻ col8)), \"): \", sum(col4 .&& col5 .&& col6 .&& (col7 .⊻ col8)) / len_vec[1], \"\\n\\n\",\n            \"15. % reverse reads of R1 in the library (\", sum(col9), \"): \", sum(col9) / len_vec[1], \"\\n\",\n            \"16. % reads passing 2, 3, 5 and 15 (\", sum(col4 .&& col5 .&& col6 .&& col9), \"): \", sum(col4 .&& col5 .&& col6 .&& col9) / len_vec[1], \"\\n\\n\",\n            \"17. % complementary reads of R1 in the library (\", sum(col10), \"): \", sum(col10) / len_vec[1], \"\\n\",\n            \"18. % reads passing 2, 3, 5 and 17 (\", sum(col4 .&& col5 .&& col6 .&& col10), \"): \", sum(col4 .&& col5 .&& col6 .&& col10) / len_vec[1], \"\\n\\n\",\n            \"19. % reverse reads of R2 in the library (\", sum(col11), \"): \", sum(col11) / len_vec[1], \"\\n\",\n            \"20. % reads passing 2, 3, 5 and 19 (\", sum(col4 .&& col5 .&& col6 .&& col11), \"): \", sum(col4 .&& col5 .&& col6 .&& col11) / len_vec[1], \"\\n\\n\",\n            \"21. % complementary reads of R2 in the library (\", sum(col12), \"): \", sum(col12) / len_vec[1], \"\\n\",\n            \"22. % reads passing 2, 3, 5 and 21 (\", sum(col4 .&& col5 .&& col6 .&& col12), \"): \", sum(col4 .&& col5 .&& col6 .&& col12) / len_vec[1], \"\\n\\n\\n\"\n        )\n        print(io, stat_str)\n    end\n\n    # Filter out invalid Barcode-UMI pairs\n    valid_bc_umi_vec = bc_umi_vec[col4.&&col5.&&col6.&&(col7.⊻col8)]\n    df_dict[unique(read_df[:, :tech_sample])[1]] = DataFrame(\n        [[(x[1], x[3]) for x in valid_bc_umi_vec[getindex.(valid_bc_umi_vec, 7)]]; [(x[2], x[3]) for x in valid_bc_umi_vec[getindex.(valid_bc_umi_vec, 8)]]],\n        [:barcode, :umi]\n    )\n    df_dict[unique(read_df[:, :tech_sample])[1]][!, :tech_sample] .= unique(read_df[:, :tech_sample])[1]\nend\nbc_umi_df = reduce(vcat, collect(values(df_dict)); cols=:setequal)\nCSV.write(joinpath(raw_bc_umi_dir, \"raw_bc_umi_pairs.tsv\"), bc_umi_df; header=true, delim=\"\\t\", append=false)\n```\n:::\n\n\n#### Solution 2\n\n::: {#6582e8d5 .cell execution_count=15}\n``` {.julia .cell-code}\nusing FASTX, DataFrames, CSV, YRUtils, CodecZlib\n\nraw_fastq_dir = \"raw_fastq\"\nextract_valid_barcode_dir = \"extract_valid_barcode\"\nbase_qual = 20\nseqkit_nthreads = 40\nparallel_block_size = 500000\n\nmkpath(extract_valid_barcode_dir)\nraw_fastq_file_name_pattern = r\".+/(?<id>[a-zA-Z0-9]+)_(?<type>RNA|DNA)_(?<rep>rep[0-9]+)(_(?<tech>part[0-9]+))?\\.(?<read>R[123])\\.(fq|fastq)\\.gz$\"\nraw_fastq_files = YRUtils.BioUtils.list_files(raw_fastq_dir, raw_fastq_file_name_pattern; recursive=false, full_name=true)\nraw_fastq_nums = YRUtils.BioUtils.fq_num(raw_fastq_files, seqkit_nthreads)\nms = match.(raw_fastq_file_name_pattern, raw_fastq_files)\nmetadata_vec = Vector{NTuple{7,String}}(undef, length(ms))\nfor i in 1:length(ms)\n    metadata_vec[i] = (ms[i].match, ms[i][\"id\"], ms[i][\"type\"], ms[i][\"rep\"], ms[i][\"tech\"], ms[i][\"read\"], string(raw_fastq_nums[ms[i].match]))\nend\ndf = unique(DataFrame(metadata_vec, [:file, :id, :type, :rep, :tech, :read, :num_seqs]))\ndf = transform(\n    df,\n    [:id, :type, :rep, :tech] => ByRow((id, type, rep, tech) -> join([id, type, rep, tech], \"_\")) => :tech_sample,\n    [:id, :type, :rep] => ByRow((id, type, rep) -> join([id, type, rep], \"_\")) => :rep_sample,\n    [:id, :type] => ByRow((id, type) -> join([id, type], \"_\")) => :type_sample\n)\nCSV.write(joinpath(extract_valid_barcode_dir, \"fq_metadata.tsv\"), df; header=true, delim=\"\\t\", append=false)\n\n# global var\ntop_dict = Dict{String,Set{String}}()\nfor (type, type_subdf) in pairs(groupby(df, :type))\n    global top_dict\n    # local var\n    type_dict = Dict{String,Set{String}}()\n    for (biorep, biorep_subdf) in pairs(groupby(type_subdf, :rep))\n        # local var\n        biorep_df = DataFrame(barcode=String[], count=Int64[])\n        biorep_dict = Dict{String,Set{String}}()\n        for (techrep, techrep_subdf) in pairs(groupby(biorep_subdf, :tech))\n            # local var\n            techrep_dict = Dict{String,Set{String}}()\n\n            # Read in reads\n            read_dict = Dict(read_type => Vector{Tuple{String,String,String}}(undef, parse(Int64, num_seqs)) for (read_type, num_seqs) in collect(zip(techrep_subdf[:, :read], techrep_subdf[:, :num_seqs])))\n            Threads.@threads for (read_type, num_seqs, fq_file) in collect(zip(techrep_subdf[:, :read], techrep_subdf[:, :num_seqs], techrep_subdf[:, :file]))\n                @info string(\"start parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" ...\")\n                FASTQReader(GzipDecompressorStream(open(fq_file))) do reader\n                    record = FASTQ.Record()\n                    i = 0\n                    while !eof(reader)\n                        i += 1\n                        empty!(record)\n                        read!(reader, record)\n                        read_dict[read_type][i] = (identifier(record), sequence(record), join(collect(quality_scores(record)), \"/\"))\n                    end\n                    @info string(\"read in \", i, \" sequences in total for \", fq_file)\n                    if i != parse(Int64, num_seqs)\n                        @error string(\"parsing file \", fq_file, \" failed!\")\n                    end\n                end\n                @info string(\"parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" done!\")\n            end\n\n            # Count qualified barcodes and their UMIs\n            len_vec = length.(values(read_dict))\n            # The three files should have the same number of lines\n            if length(unique(len_vec)) == 1\n                bc_umi_vec = Vector{Tuple{String,String,String,Vararg{Bool,3}}}(undef, len_vec[1])\n                Threads.@threads for i in 1:len_vec[1]\n                    # Read IDs should be identical across R1, R2, and R3\n                    if read_dict[\"R1\"][i][1] == read_dict[\"R2\"][i][1] == read_dict[\"R3\"][i][1]\n                        bc_umi_vec[i] = (\n                            # Read 1\n                            read_dict[\"R1\"][i][2],\n                            # Read 2\n                            read_dict[\"R2\"][i][2],\n                            # UMI\n                            read_dict[\"R3\"][i][2],\n                            # Read sequences should only contain A, T, C, and G across R1, R2, and R3\n                            !occursin(\"N\", string(read_dict[\"R1\"][i][2], read_dict[\"R2\"][i][2], read_dict[\"R3\"][i][2])),\n                            # All base qualities >= base_qual across R1, R2, and R3\n                            all(parse.(Int, split(string(read_dict[\"R1\"][i][3], \"/\", read_dict[\"R2\"][i][3], \"/\", read_dict[\"R3\"][i][3]), \"/\")) .>= base_qual),\n                            # Read 1 and read 2 should be reverse and complementary\n                            YRUtils.BioUtils.rev_com_dna_seq(read_dict[\"R1\"][i][2]) == read_dict[\"R2\"][i][2]\n                        )\n                    else\n                        @error \"read IDs are not identical across R1, R2, and R3\"\n                    end\n                end\n            else\n                @error \"length(R1) == length(R2) == length(R3) is not true\"\n            end\n            read_dict = nothing\n            GC.gc()\n\n            # Filter out invalid Barcode-UMI pairs\n            valid_bc_umi_vec = bc_umi_vec[getindex.(bc_umi_vec, 4).&&getindex.(bc_umi_vec, 5).&&getindex.(bc_umi_vec, 6)]\n            bc_umi_vec = nothing\n            GC.gc()\n\n            split_valid_bc_umi_vec = collect(Base.Iterators.partition(valid_bc_umi_vec, parallel_block_size))\n            techrep_dict_vec = [Dict{String,Set{String}}() for _ in 1:length(split_valid_bc_umi_vec)]\n            Threads.@threads for i in 1:length(split_valid_bc_umi_vec)\n                for item in split_valid_bc_umi_vec[i]\n                    if haskey(techrep_dict_vec[i], item[1])\n                        push!(techrep_dict_vec[i][item[1]], item[3])\n                    elseif haskey(techrep_dict_vec[i], item[2])\n                        push!(techrep_dict_vec[i][item[2]], item[3])\n                    else\n                        techrep_dict_vec[i][item[1]] = Set([item[3]])\n                    end\n                end\n            end\n            valid_bc_umi_vec = nothing\n            split_valid_bc_umi_vec = nothing\n            GC.gc()\n\n            while true\n                if length(techrep_dict_vec) <= 1\n                    techrep_dict = techrep_dict_vec[1]\n                    break\n                else\n                    split_techrep_dict_vec = convert(Vector{Vector{Dict{String,Set{String}}}}, collect(Base.Iterators.partition(techrep_dict_vec, 2)))\n                end\n\n                if length(split_techrep_dict_vec[end]) == 1\n                    push!(split_techrep_dict_vec[end-1], pop!(split_techrep_dict_vec[end]))\n                    pop!(split_techrep_dict_vec)\n                end\n\n                Threads.@threads for i in 1:length(split_techrep_dict_vec)\n                    for j in 2:length(split_techrep_dict_vec[i])\n                        for (k, v) in split_techrep_dict_vec[i][j]\n                            if haskey(split_techrep_dict_vec[i][1], k)\n                                split_techrep_dict_vec[i][1][k] = union(split_techrep_dict_vec[i][1][k], v)\n                            elseif haskey(split_techrep_dict_vec[i][1], YRUtils.BioUtils.rev_com_dna_seq(k))\n                                split_techrep_dict_vec[i][1][YRUtils.BioUtils.rev_com_dna_seq(k)] = union(split_techrep_dict_vec[i][1][YRUtils.BioUtils.rev_com_dna_seq(k)], v)\n                            else\n                                split_techrep_dict_vec[i][1][k] = v\n                            end\n                        end\n                    end\n                end\n\n                techrep_dict_vec = getindex.(split_techrep_dict_vec, 1)\n            end\n\n            open(joinpath(extract_valid_barcode_dir, \"fq_read_stats.txt\"), \"a\") do io\n                stat_str = string(\n                    techrep_subdf.tech_sample[1], \": \", length(techrep_dict), \"\\n\")\n                print(io, stat_str)\n            end\n\n            if isempty(biorep_dict)\n                biorep_dict = techrep_dict\n            else\n                for (k, v) in techrep_dict\n                    if haskey(biorep_dict, k)\n                        biorep_dict[k] = union(biorep_dict[k], v)\n                    elseif haskey(biorep_dict, YRUtils.BioUtils.rev_com_dna_seq(k))\n                        biorep_dict[YRUtils.BioUtils.rev_com_dna_seq(k)] = union(biorep_dict[YRUtils.BioUtils.rev_com_dna_seq(k)], v)\n                    else\n                        biorep_dict[k] = v\n                    end\n                end\n            end\n        end\n\n        biorep_df = sort(DataFrame([(barcode=k, count=length(v)) for (k, v) in biorep_dict]), :count, rev=true)\n        CSV.write(joinpath(extract_valid_barcode_dir, string(biorep_subdf.rep_sample[1], \".barcode_count.tsv\")), biorep_df; header=true, delim=\"\\t\", append=false)\n\n        open(joinpath(extract_valid_barcode_dir, \"fq_read_stats.txt\"), \"a\") do io\n            stat_str = string(\n                biorep_subdf.rep_sample[1], \": \", length(biorep_dict), \"\\n\")\n            print(io, stat_str)\n        end\n\n        if isempty(type_dict)\n            type_dict = biorep_dict\n        else\n            for (k, v) in biorep_dict\n                if haskey(type_dict, k)\n                    type_dict[k] = union(type_dict[k], v)\n                elseif haskey(type_dict, YRUtils.BioUtils.rev_com_dna_seq(k))\n                    type_dict[YRUtils.BioUtils.rev_com_dna_seq(k)] = union(type_dict[YRUtils.BioUtils.rev_com_dna_seq(k)], v)\n                else\n                    type_dict[k] = v\n                end\n            end\n        end\n    end\n\n    open(joinpath(extract_valid_barcode_dir, \"fq_read_stats.txt\"), \"a\") do io\n        stat_str = string(\n            type_subdf.type[1], \": \", length(type_dict), \"\\n\")\n        print(io, stat_str)\n    end\n\n    if isempty(top_dict)\n        top_dict = type_dict\n    else\n        for (k, v) in type_dict\n            global top_dict\n            if haskey(top_dict, k)\n                top_dict[k] = union(top_dict[k], v)\n            elseif haskey(top_dict, YRUtils.BioUtils.rev_com_dna_seq(k))\n                top_dict[YRUtils.BioUtils.rev_com_dna_seq(k)] = union(top_dict[YRUtils.BioUtils.rev_com_dna_seq(k)], v)\n            else\n                top_dict[k] = v\n            end\n        end\n    end\nend\n\nopen(joinpath(extract_valid_barcode_dir, \"fq_read_stats.txt\"), \"a\") do io\n    stat_str = string(\n        \"ALL: \", length(top_dict), \"\\n\")\n    print(io, stat_str)\nend\n```\n:::\n\n\n### Attach barcodes to CREs\n\n#### Solution 1\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggprism)\nlibrary(YRUtils)\n\nraw_bc_umi_pairs_file <- \"raw_bc_umi/raw_bc_umi_pairs.tsv\"\n# Here, we only keep those CRE-Barcode pairs, where each barcode is assigned to only one CRE\ncre_bc_file <- \"/data/users/dell/mpra/link_barcode_to_cre/final_result/non_redundant_cre_bc_pairs.tsv\"\noutput_dir <- \"cre_bc_count\"\n\ndir.create(output_dir)\ncre_bc_df <- vroom(cre_bc_file)\nraw_bc_umi_pairs_df <- vroom(raw_bc_umi_pairs_file)\n\n# Count barcodes based on UMIs\nbiorep_bc_count_df <- raw_bc_umi_pairs_df %>%\n    mutate(rep_sample = gsub(\"_part[0-9]+$\", \"\", tech_sample)) %>%\n    select(-all_of(c(\"tech_sample\"))) %>%\n    distinct() %>%\n    group_by(rep_sample, barcode) %>%\n    # Count the number of occurrences of each barcode in each biological replicate\n    count(name = \"barcode_count\") %>%\n    ungroup() %>%\n    group_by(rep_sample) %>%\n    arrange(desc(barcode_count), .by_group = TRUE) %>%\n    ungroup()\nvroom_write(biorep_bc_count_df, file = file.path(output_dir, \"biorep_bc_count.tsv\"))\n\n# Attach CREs to barcodes\nbiorep_cre_bc_count_df <- biorep_bc_count_df %>%\n    inner_join(cre_bc_df, by = \"barcode\", relationship = \"many-to-many\") %>%\n    group_by(rep_sample) %>%\n    arrange(desc(barcode_count), .by_group = TRUE) %>%\n    ungroup()\nvroom_write(biorep_cre_bc_count_df, file = file.path(output_dir, \"biorep_cre_bc_count.tsv\"))\n\n# Count the number of unique barcodes detected in each biological replicate\nbc_num_each_biorep <- biorep_bc_count_df %>%\n    group_by(rep_sample) %>%\n    count(name = \"barcode_num\") %>%\n    ungroup()\nvroom_write(bc_num_each_biorep, file = file.path(output_dir, \"bc_num_each_biorep.tsv\"))\n\n# The distribution of the number of barcodes belonging to each CRE\n# The distribution of the number of CREs belonging to each barcode\nleft_cre_bc_df <- biorep_cre_bc_count_df %>%\n    select(barcode, cre) %>%\n    distinct()\nbc_count_df <- left_cre_bc_df %>%\n    group_by(barcode) %>%\n    count() %>%\n    rename(cre_bc = barcode) %>%\n    mutate(type = \"BC\")\ncre_count_df <- left_cre_bc_df %>%\n    group_by(cre) %>%\n    count() %>%\n    rename(cre_bc = cre) %>%\n    mutate(type = if_else(str_detect(cre_bc, \"^CTL\"), \"CTL\", \"CRE\"))\ncount_df <- bind_rows(cre_count_df, bc_count_df)\n\ncre_quantiles <- quantile(count_df$n[count_df$type %in% c(\"CTL\", \"CRE\")], probs = seq(0, 1, 0.1))\ncre_only_quantiles <- quantile(count_df$n[count_df$type == \"CRE\"], probs = seq(0, 1, 0.1))\nctl_only_quantiles <- quantile(count_df$n[count_df$type == \"CTL\"], probs = seq(0, 1, 0.1))\nbc_quantiles <- quantile(count_df$n[count_df$type == \"BC\"], probs = seq(0, 1, 0.1))\ntype_nums <- table(count_df$type)\n\npaste0(\n    \"1. CRE/CTL quantiles (\", type_nums[\"CTL\"] + type_nums[\"CRE\"], \"): \\n\",\n    paste0(paste0(names(cre_quantiles), \"\\t\", cre_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"2. CRE only quantiles (\", type_nums[\"CRE\"], \"): \\n\",\n    paste0(paste0(names(cre_only_quantiles), \"\\t\", cre_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"3. CTL only quantiles (\", type_nums[\"CTL\"], \"): \\n\",\n    paste0(paste0(names(ctl_only_quantiles), \"\\t\", ctl_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"4. BC quantiles (\", type_nums[\"BC\"], \"): \\n\",\n    paste0(paste0(names(bc_quantiles), \"\\t\", bc_quantiles), collapse = \"\\n\")\n) %>% vroom_write_lines(file = file.path(output_dir, \"quantiles.txt\"))\n\np <- ggplot(count_df, aes(type, log2(n), fill = type, color = type)) +\n    geom_violin(scale = \"width\", alpha = 0.25, trim = TRUE) +\n    geom_boxplot(width = 0.2, outliers = FALSE, alpha = 0.25) +\n    scale_y_continuous(expand = expansion(mult = c(0.05, 0))) +\n    labs(\n        x = \"Sequence Type\",\n        y = \"log2(Count)\"\n    ) +\n    theme_prism(base_size = 20, base_family = \"Arial\", border = FALSE) +\n    theme(legend.position = \"none\")\nppreview(p, file = file.path(output_dir, \"violin.pdf\"))\n```\n\n\n\n\n#### Solution 2\n\n::: {#702a972d .cell execution_count=16}\n``` {.julia .cell-code}\nusing CSV, DataFrames, YRUtils\n\nextract_valid_barcode_dir = \"extract_valid_barcode\"\noutput_dir = \"cre_bc_count\"\ncre_bc_file = \"/data/users/dell/mpra/link_barcode_to_cre/final_result/non_redundant_cre_bc_pairs.tsv\"\n\nmkpath(output_dir)\ncre_bc_df = CSV.read(cre_bc_file, DataFrame; header=true, delim=\"\\t\")\nbarcode_count_file_name_pattern = r\".+/(?<id>[a-zA-Z0-9]+)_(?<type>RNA|DNA)_(?<rep>rep[0-9]+)\\.barcode_count\\.(txt|tsv)$\"\nbarcode_count_files = YRUtils.BioUtils.list_files(extract_valid_barcode_dir, barcode_count_file_name_pattern; recursive=false, full_name=true)\nms = match.(barcode_count_file_name_pattern, barcode_count_files)\nmetadata_vec = Vector{NTuple{4,String}}(undef, length(ms))\nfor i in 1:length(ms)\n    metadata_vec[i] = (ms[i].match, ms[i][\"id\"], ms[i][\"type\"], ms[i][\"rep\"])\nend\ndf = unique(DataFrame(metadata_vec, [:file, :id, :type, :rep]))\ndf = transform(\n    df,\n    [:id, :type, :rep] => ByRow((id, type, rep) -> join([id, type, rep], \"_\")) => :rep_sample,\n    [:id, :type] => ByRow((id, type) -> join([id, type], \"_\")) => :type_sample\n)\nCSV.write(joinpath(output_dir, \"barcode_count_file_metadata.tsv\"), df; header=true, delim=\"\\t\", append=false)\n\ncre_barcode_count_df = crossjoin(DataFrame(cre=String[], barcode=String[], value_barcode=String[], count=Int64[], CPM=Float64[]), empty(df))\nfor row_subdf in eachrow(df)\n    global cre_barcode_count_df\n    barcode_count_df = CSV.read(row_subdf.file, DataFrame; header=true, delim=\"\\t\")\n    barcode_count_df = crossjoin(barcode_count_df, DataFrame(row_subdf))\n    barcode_count_df.CPM = barcode_count_df.count * 1000000 / sum(barcode_count_df.count)\n\n    in_barcode_set = in(Set(barcode_count_df.barcode))\n    cre_barcode_dict = Dict(k => \"\" for k in cre_bc_df.barcode)\n    Threads.@threads for k in cre_bc_df.barcode\n        if in_barcode_set(k)\n            cre_barcode_dict[k] = k\n        elseif in_barcode_set(YRUtils.BioUtils.rev_com_dna_seq(k))\n            cre_barcode_dict[k] = YRUtils.BioUtils.rev_com_dna_seq(k)\n        end\n    end\n    bridge_barcode_df = DataFrame([(key_barcode=k, value_barcode=v) for (k, v) in cre_barcode_dict])\n    bridge_barcode_df = filter(:value_barcode => x -> x != \"\", bridge_barcode_df)\n\n    barcode_count_df = innerjoin(bridge_barcode_df, barcode_count_df, on=:value_barcode => :barcode)\n    barcode_count_df = innerjoin(cre_bc_df, barcode_count_df, on=:barcode => :key_barcode)\n    append!(cre_barcode_count_df, barcode_count_df; cols=:setequal)\nend\nCSV.write(joinpath(output_dir, \"cre_barcode_count.tsv\"), cre_barcode_count_df; header=true, delim=\"\\t\", append=false)\n```\n:::\n\n\n## Identify active CREs\n\n### Q95 threshold method\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\n\ncre_bc_count_file <- \"cre_bc_count/cre_barcode_count.tsv\"\noutput_dir <- \"identify_active.Q95\"\n\ndir.create(output_dir)\ncre_bc_count_df <- vroom(cre_bc_count_file)\ncre_bc_cpm_df <- pivot_wider(cre_bc_count_df, id_cols = c(\"rep\", \"cre\", \"barcode\"), names_from = \"type\", values_from = \"CPM\", values_fill = 0) %>%\n    filter(DNA > 0)\n\ncre_bc_qs <- cre_bc_cpm_df %>%\n    group_by(rep, cre) %>%\n    reframe(n = n()) %>%\n    pull(n) %>%\n    quantile(probs = seq(0, 1, 0.05))\nsink(file = file.path(output_dir, \"cre_bc_quantiles.txt\"))\nprint(cre_bc_qs)\nsink()\n\ncre_bc_cpm_df <- cre_bc_cpm_df %>%\n    group_by(rep, cre) %>%\n    reframe(DNA = sum(DNA), RNA = sum(RNA)) %>%\n    mutate(\n        activity = RNA / DNA,\n        cre_type = if_else(str_detect(cre, \"^PSCE\\\\d+(_\\\\d+)?$\"), \"PSCE\",\n            if_else(str_detect(cre, \"^CTL\\\\d+(_\\\\d+)?\"), \"CTL\", \"NA\")\n        ),\n        cre_type = factor(cre_type, levels = c(\"CTL\", \"PSCE\")),\n        rep = factor(rep, levels = c(\"rep1\", \"rep2\"))\n    )\nif (any(is.na(cre_bc_cpm_df$cre_type))) {\n    stop(\"unexpected CRE type found\")\n}\n\np_density <- ggplot(cre_bc_cpm_df, aes(log2(activity + 1), fill = cre_type, color = cre_type)) +\n    geom_density(alpha = 0.1) +\n    scale_y_continuous(expand = expansion(mult = 0.02)) +\n    facet_grid(. ~ rep) +\n    labs(color = \"CRE Type\", fill = \"CRE Type\") +\n    theme_classic(base_size = 20, base_family = \"Arial\") +\n    theme(strip.background = element_blank())\nppreview(p_density, file = file.path(output_dir, \"density.pdf\"))\n\ncre_bc_cpm_ls <- split(cre_bc_cpm_df, cre_bc_cpm_df$rep)\ncre_bc_cpm_ls <- lapply(cre_bc_cpm_ls, function(df) {\n    ctl_activity_qs <- df %>%\n        filter(cre_type == \"CTL\") %>%\n        pull(activity) %>%\n        quantile(probs = seq(0, 1, 0.05))\n    psce_activity_qs <- df %>%\n        filter(cre_type == \"PSCE\") %>%\n        pull(activity) %>%\n        quantile(probs = seq(0, 1, 0.05))\n    psce_df <- df %>%\n        filter(cre_type == \"PSCE\") %>%\n        mutate(is_active = if_else(activity > ctl_activity_qs[\"95%\"], TRUE, FALSE)) %>%\n        arrange(desc(is_active), desc(activity))\n    p_violin <- ggplot(df, aes(\n        cre_type, activity,\n        fill = cre_type, color = cre_type\n    )) +\n        geom_violin(trim = TRUE, scale = \"width\", alpha = 0.5) +\n        scale_y_continuous(expand = expansion(mult = 0.02)) +\n        labs(x = NULL, y = \"Activity\") +\n        theme_classic(base_family = \"Arial\", base_size = 20) +\n        theme(legend.position = \"none\")\n    p_density <- ggplot(df, aes(log2(activity + 1), fill = cre_type, color = cre_type)) +\n        geom_density(alpha = 0.1) +\n        geom_vline(xintercept = ctl_activity_qs[\"95%\"], color = \"grey\", linetype = \"dashed\") +\n        scale_y_continuous(expand = expansion(mult = 0.02)) +\n        labs(color = \"CRE Type\", fill = \"CRE Type\") +\n        theme_classic(base_size = 20, base_family = \"Arial\")\n\n    list(\n        df = df %>% arrange(cre_type, desc(activity)),\n        ctl_activity_qs = ctl_activity_qs,\n        psce_activity_qs = psce_activity_qs,\n        psce_df = psce_df,\n        violin = p_violin,\n        density = p_density\n    )\n})\nfor (rep in names(cre_bc_cpm_ls)) {\n    vroom_write(cre_bc_cpm_ls[[rep]]$df, file = file.path(output_dir, paste0(rep, \".df.tsv\")))\n    vroom_write(cre_bc_cpm_ls[[rep]]$psce_df, file = file.path(output_dir, paste0(rep, \".psce_df.tsv\")))\n    sink(file = file.path(output_dir, paste0(rep, \".ctl_activity_quantiles.txt\")))\n    print(cre_bc_cpm_ls[[rep]]$ctl_activity_qs)\n    sink()\n    sink(file = file.path(output_dir, paste0(rep, \".psce_activity_quantiles.txt\")))\n    print(cre_bc_cpm_ls[[rep]]$psce_activity_qs)\n    sink()\n    ppreview(cre_bc_cpm_ls[[rep]]$violin, file = file.path(output_dir, paste0(rep, \".violin.pdf\")))\n    ppreview(cre_bc_cpm_ls[[rep]]$density, file = file.path(output_dir, paste0(rep, \".density.pdf\")))\n}\n\njoin_df <- full_join(\n    cre_bc_cpm_ls$rep1$psce_df %>%\n        filter(is_active) %>%\n        select(cre, activity, is_active),\n    cre_bc_cpm_ls$rep2$psce_df %>%\n        filter(is_active) %>%\n        select(cre, activity, is_active),\n    by = \"cre\", suffix = c(\".rep1\", \".rep2\")\n) %>%\n    mutate(both_are_active = if_else(is.na(is_active.rep1), \"rep2\",\n        if_else(is.na(is_active.rep2), \"rep1\", \"both\")\n    )) %>%\n    arrange(both_are_active)\nvroom_write(join_df, file = file.path(output_dir, \"rep1_rep2.psce_df.tsv\"))\n```\n\n\n\n\n### MPRAnalyze method\n\n\n\n\n```{r}\nlibrary(MPRAnalyze)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(magrittr)\n\ncre_bc_count_file <- \"cre_bc_count/cre_barcode_count.tsv\"\noutput_dir <- \"identify_active.MPRAnalyze\"\n\ndir.create(output_dir)\ncre_bc_count_df <- vroom(cre_bc_count_file)\ncre_bc_count_df <- pivot_wider(cre_bc_count_df, id_cols = c(\"rep\", \"cre\", \"barcode\"), names_from = \"type\", values_from = \"count\", values_fill = 0) %>%\n    filter(DNA > 0) %>%\n    group_by(rep, cre) %>%\n    reframe(DNA = sum(DNA), RNA = sum(RNA)) %>%\n    mutate(\n        cre_type = if_else(str_detect(cre, \"^PSCE\\\\d+(_\\\\d+)?$\"), \"PSCE\",\n            if_else(str_detect(cre, \"^CTL\\\\d+(_\\\\d+)?\"), \"CTL\", \"NA\")\n        )\n    )\ndna_count_df <- pivot_wider(cre_bc_count_df, id_cols = c(\"cre\", \"cre_type\"), names_from = \"rep\", values_from = \"DNA\", values_fill = 0) %>%\n    arrange(cre) %>%\n    as.data.frame() %>%\n    set_rownames(.[[\"cre\"]])\nrna_count_df <- pivot_wider(cre_bc_count_df, id_cols = c(\"cre\", \"cre_type\"), names_from = \"rep\", values_from = \"RNA\", values_fill = 0) %>%\n    arrange(cre) %>%\n    as.data.frame() %>%\n    set_rownames(.[[\"cre\"]])\nif (!all(row.names(dna_count_df) == row.names(rna_count_df))) {\n    stop(\"the row names of DNA are not identical to the row names of RNA\")\n}\n\ncol_names <- c(\"rep1\", \"rep2\")\nobj <- MpraObject(\n    dnaCounts = as.matrix(dna_count_df[, col_names]),\n    rnaCounts = as.matrix(rna_count_df[, col_names]),\n    dnaAnnot = data.frame(replicate = col_names, row.names = col_names),\n    rnaAnnot = data.frame(replicate = col_names, row.names = col_names),\n    controls = dna_count_df$cre_type == \"CTL\"\n)\nobj <- estimateDepthFactors(obj, lib.factor = \"replicate\", which.lib = \"dna\", depth.estimator = \"uq\")\nobj <- estimateDepthFactors(obj, lib.factor = \"replicate\", which.lib = \"rna\", depth.estimator = \"uq\")\nobj <- analyzeQuantification(obj, dnaDesign = ~replicate, rnaDesign = ~1)\n\nalpha_df <- getAlpha(obj) %>%\n    mutate(\n        cre = row.names(.),\n        cre_type = if_else(str_detect(cre, \"^PSCE\\\\d+(_\\\\d+)?$\"), \"PSCE\",\n            if_else(str_detect(cre, \"^CTL\\\\d+(_\\\\d+)?\"), \"CTL\", \"NA\")\n        ),\n        cre_type = factor(cre_type, levels = c(\"CTL\", \"PSCE\"))\n    )\np_violin <- ggplot(alpha_df, aes(\n    cre_type, alpha,\n    fill = cre_type, color = cre_type\n)) +\n    geom_violin(trim = TRUE, scale = \"width\", alpha = 0.5) +\n    scale_y_continuous(expand = expansion(mult = 0.02)) +\n    labs(x = NULL, y = \"Activity\") +\n    theme_classic(base_family = \"Arial\", base_size = 20) +\n    theme(legend.position = \"none\")\nppreview(p_violin, file = file.path(output_dir, \"alpha.violin.pdf\"))\np_density <- ggplot(alpha_df, aes(alpha, fill = cre_type, color = cre_type)) +\n    geom_density(alpha = 0.1) +\n    scale_y_continuous(expand = expansion(mult = 0.02)) +\n    labs(color = \"CRE Type\", fill = \"CRE Type\") +\n    theme_classic(base_size = 20, base_family = \"Arial\")\nppreview(p_density, file = file.path(output_dir, \"alpha.density.pdf\"))\n\nres_df <- testEmpirical(obj, statistic = alpha_df$alpha)\nres_df <- bind_cols(alpha_df, res_df) %>%\n    mutate(\n        pval.empirical.adj = p.adjust(pval.empirical, method = \"fdr\"),\n        pval.mad.adj = p.adjust(pval.mad, method = \"fdr\"),\n        pval.zscore.adj = p.adjust(pval.zscore, method = \"fdr\")\n    ) %>%\n    filter(cre_type == \"PSCE\") %>%\n    arrange(pval.mad.adj) %>%\n    mutate(is_active = if_else(pval.mad.adj < 0.05, TRUE, FALSE))\nvroom_write(res_df, file = file.path(output_dir, \"psce_df.tsv\"))\n```\n\n\n\n\n## Peak annotation\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\nlibrary(YRUtils)\nlibrary(ChIPseeker)\nlibrary(AnnotationDbi)\nlibrary(patchwork)\n\nmpt_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.trna.ercc.phix.gtf.gz.gene_id_name_mapping_table.tsv\"\ntxdb_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\nmpra_test_file <- \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20250430/ref/2w_library.165bp.no_enzyme_cutting_sites.dealed.tsv\"\nrep1_file <- \"identify_active.Q95/rep1.psce_df.tsv\"\nrep2_file <- \"identify_active.Q95/rep2.psce_df.tsv\"\nmpranalyze_file <- \"identify_active.MPRAnalyze/psce_df.tsv\"\noutput_dir <- \"identify_active.overlap\"\n\ndir.create(output_dir)\nmpt_df <- vroom(mpt_file) %>%\n    dplyr::select(gene_id, gene_name, gene_biotype, gene_version, gene_level)\ntxdb <- loadDb(txdb_file)\nmpra_test_df <- vroom(mpra_test_file) %>%\n    dplyr::select(seqname, start, end, PSCE, PSCE_new_id) %>%\n    mutate(id = paste0(seqname, \":\", start, \"-\", end))\nrep1_df <- vroom(rep1_file) %>%\n    filter(is_active) %>%\n    dplyr::select(-rep, -cre_type)\nrep2_df <- vroom(rep2_file) %>%\n    filter(is_active) %>%\n    dplyr::select(-rep, -cre_type)\nmpranalyze_df <- vroom(mpranalyze_file) %>%\n    filter(is_active) %>%\n    dplyr::select(-cre_type)\nnames(mpranalyze_df) <- paste0(names(mpranalyze_df), \".mpranalyze\")\n\njoin_df <- full_join(rep1_df, rep2_df, by = \"cre\", suffix = c(\".rep1\", \".rep2\")) %>%\n    full_join(mpranalyze_df, by = c(\"cre\" = \"cre.mpranalyze\")) %>%\n    arrange(pval.mad.adj.mpranalyze) %>%\n    inner_join(mpra_test_df, by = c(\"cre\" = \"PSCE_new_id\"))\nvroom_write(join_df, file = file.path(output_dir, \"overlap.df.tsv\"))\n\nls <- list(rep1 = rep1_df$cre, rep2 = rep2_df$cre, mpranalyze = mpranalyze_df$cre.mpranalyze)\np_venn <- ggVennDiagram(ls, label_alpha = 0, label = \"count\", label_size = 12) +\n    scale_fill_gradient(low = \"grey90\", high = \"#FF7256\") +\n    xlim(c(-10, 10)) +\n    theme(legend.position = \"none\")\nppreview(p_venn, file = file.path(output_dir, \"overlap.venn.pdf\"))\n\ngr <- join_df[, c(\"seqname\", \"start\", \"end\", \"cre\")] %>%\n    as(\"GRanges\")\npeak_anno <- annotatePeak(gr, tssRegion = c(-3000, 3000), TxDb = txdb)\n\np_bar <- plotAnnoBar(peak_anno) +\n    labs(title = NULL) +\n    theme(text = element_text(family = \"Arial\", size = 20))\nppreview(p_bar, file = file.path(output_dir, \"peak_anno.feature_distribution.pdf\"))\n\np_tss <- plotDistToTSS(peak_anno) +\n    labs(title = NULL) +\n    theme(text = element_text(family = \"Arial\", size = 20))\nppreview(p_tss, file = file.path(output_dir, \"peak_anno.distance_to_tss.pdf\"))\n\npeak_anno_df <- as.data.frame(peak_anno) %>%\n    dplyr::select(cre, annotation, geneId, distanceToTSS) %>%\n    inner_join(mpt_df, by = c(\"geneId\" = \"gene_id\"))\nfinal_df <- inner_join(join_df, peak_anno_df, by = \"cre\")\nvroom_write(final_df, file = file.path(output_dir, \"overlap.peak_anno.df.tsv\"))\n\ngene_list_df <- final_df %>%\n    dplyr::select(gene_name) %>%\n    distinct() %>%\n    mutate(cluster = \"active_psce\")\nvroom_write(gene_list_df, file.path(output_dir, \"overlap.peak_anno.gene_list.df.tsv\"))\n```\n\n\n\n\n## Appendix\n\n### Identify active barcodes\n\n#### Extract barcode-sample matrix\n\n::: {#4676a754 .cell execution_count=17}\n``` {.julia .cell-code}\nusing CSV, DataFrames, YRUtils\n\nextract_valid_barcode_dir = \"extract_valid_barcode\"\noutput_dir = \"bc_count\"\n\nmkpath(output_dir)\nbarcode_count_file_name_pattern = r\".+/(?<id>[a-zA-Z0-9]+)_(?<type>RNA|DNA)_(?<rep>rep[0-9]+)\\.barcode_count\\.(txt|tsv)$\"\nbarcode_count_files = YRUtils.BioUtils.list_files(extract_valid_barcode_dir, barcode_count_file_name_pattern; recursive=false, full_name=true)\nms = match.(barcode_count_file_name_pattern, barcode_count_files)\nmetadata_vec = Vector{NTuple{4,String}}(undef, length(ms))\nfor i in 1:length(ms)\n    metadata_vec[i] = (ms[i].match, ms[i][\"id\"], ms[i][\"type\"], ms[i][\"rep\"])\nend\ndf = unique(DataFrame(metadata_vec, [:file, :id, :type, :rep]))\ndf = transform(\n    df,\n    [:id, :type, :rep] => ByRow((id, type, rep) -> join([id, type, rep], \"_\")) => :rep_sample,\n    [:id, :type] => ByRow((id, type) -> join([id, type], \"_\")) => :type_sample\n)\nCSV.write(joinpath(output_dir, \"barcode_count_file_metadata.tsv\"), df; header=true, delim=\"\\t\", append=false)\n\nbridge_barcode_dict = Dict{String,Dict{String,String}}()\nfor row_subdf in eachrow(df)\n    global bridge_barcode_dict\n    barcode_count_df = CSV.read(row_subdf.file, DataFrame; header=true, delim=\"\\t\")\n    for barcode in barcode_count_df.barcode\n        global bridge_barcode_dict\n        if haskey(bridge_barcode_dict, barcode)\n            bridge_barcode_dict[barcode][row_subdf.rep_sample] = barcode\n        elseif haskey(bridge_barcode_dict, YRUtils.BioUtils.rev_com_dna_seq(barcode))\n            bridge_barcode_dict[YRUtils.BioUtils.rev_com_dna_seq(barcode)][row_subdf.rep_sample] = barcode\n        else\n            bridge_barcode_dict[barcode] = Dict(k => \"\" for k in df.rep_sample)\n            bridge_barcode_dict[barcode][row_subdf.rep_sample] = barcode\n        end\n    end\nend\n\nall_barcode_count_df = crossjoin(DataFrame(key_barcode=String[], value_barcode=String[], count=Int64[], CPM=Float64[]), empty(df))\nfor row_subdf in eachrow(df)\n    global bridge_barcode_dict\n    barcode_count_df = CSV.read(row_subdf.file, DataFrame; header=true, delim=\"\\t\")\n    barcode_count_df.CPM = barcode_count_df.count * 1000000 / sum(barcode_count_df.count)\n    barcode_count_df = crossjoin(barcode_count_df, DataFrame(row_subdf))\n    bridge_barcode_df = DataFrame([(key_barcode=k, value_barcode=v[row_subdf.rep_sample]) for (k, v) in bridge_barcode_dict if v[row_subdf.rep_sample] != \"\"])\n    barcode_count_df = innerjoin(bridge_barcode_df, barcode_count_df, on=:value_barcode => :barcode)\n    append!(all_barcode_count_df, barcode_count_df; cols=:setequal)\nend\nwide_barcode_count_df = unstack(all_barcode_count_df, :key_barcode, :rep_sample, :count; fill=0)\nfiltered_wide_barcode_count_df = filter([:HMC3MPRA_DNA_rep1, :HMC3MPRA_DNA_rep2] => (x, y) -> x > 0 && y > 0, wide_barcode_count_df)\nwide_barcode_cpm_df = unstack(all_barcode_count_df, :key_barcode, :rep_sample, :CPM; fill=0)\nfiltered_wide_barcode_cpm_df = filter([:HMC3MPRA_DNA_rep1, :HMC3MPRA_DNA_rep2] => (x, y) -> x > 0 && y > 0, wide_barcode_cpm_df)\nfiltered_wide_barcode_cpm_df.rep1_ratio = filtered_wide_barcode_cpm_df.HMC3MPRA_RNA_rep1 ./ filtered_wide_barcode_cpm_df.HMC3MPRA_DNA_rep1\nfiltered_wide_barcode_cpm_df.rep2_ratio = filtered_wide_barcode_cpm_df.HMC3MPRA_RNA_rep2 ./ filtered_wide_barcode_cpm_df.HMC3MPRA_DNA_rep2\n\nCSV.write(joinpath(output_dir, \"barcode_count.tsv\"), all_barcode_count_df; header=true, delim=\"\\t\", append=false)\nCSV.write(joinpath(output_dir, \"wide_barcode_count.tsv\"), wide_barcode_count_df; header=true, delim=\"\\t\", append=false)\nCSV.write(joinpath(output_dir, \"filtered_wide_barcode_count.tsv\"), filtered_wide_barcode_count_df; header=true, delim=\"\\t\", append=false)\nCSV.write(joinpath(output_dir, \"wide_barcode_cpm.tsv\"), wide_barcode_cpm_df; header=true, delim=\"\\t\", append=false)\nCSV.write(joinpath(output_dir, \"filtered_wide_barcode_cpm.tsv\"), filtered_wide_barcode_cpm_df; header=true, delim=\"\\t\", append=false)\nYRUtils.ShellUtils.recur_pigz(output_dir, r\".+\\.tsv$\"; recursive=false, pigz_options=\"\")\n```\n:::\n\n\n#### Simple threshold method\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\n\ncpm_file <- \"bc_count/filtered_wide_barcode_cpm.tsv.gz\"\noutput_dir <- \"identify_bc_active.2-fold\"\n\ndir.create(output_dir)\ncpm_df <- vroom(cpm_file) %>%\n    select(key_barcode, rep1_ratio, rep2_ratio) %>%\n    rename(Rep1 = rep1_ratio, Rep2 = rep2_ratio) %>%\n    pivot_longer(cols = c(\"Rep1\", \"Rep2\"), names_to = \"Replicate\", values_to = \"Activity\") %>%\n    mutate(log2Activity = log2(Activity + 1))\n\np_density <- ggplot(cpm_df, aes(log2Activity, fill = Replicate, color = Replicate)) +\n    geom_density(alpha = 0.1) +\n    geom_vline(xintercept = log2(3), linetype = \"dashed\", color = \"grey\") +\n    scale_y_continuous(expand = expansion(mult = 0.02)) +\n    scale_x_continuous(expand = expansion(mult = 0.02)) +\n    theme_classic(base_size = 20, base_family = \"Arial\")\nppreview(p_density, file = file.path(output_dir, \"density.pdf\"))\n\nstat_df <- cpm_df %>%\n    mutate(is_active = log2Activity > log2(3)) %>%\n    group_by(Replicate) %>%\n    reframe(n = n(), n_is_active = sum(is_active)) %>%\n    mutate(ratio = n_is_active / n)\nvroom_write(stat_df, file = file.path(output_dir, \"stat_df.tsv\"))\n```\n\n\n\n\n#### MPRAnalyze method\n\n\n\n\n```{r}\nlibrary(MPRAnalyze)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(magrittr)\nlibrary(BiocParallel)\n\ncount_file <- \"bc_count/filtered_wide_barcode_count.tsv.gz\"\noutput_dir <- \"identify_bc_active.MPRAnalyze\"\n\ndir.create(output_dir)\ncount_df <- vroom(count_file)\ndna_count_df <- count_df %>%\n    as.data.frame() %>%\n    set_rownames(.[[\"key_barcode\"]]) %>%\n    select(all_of(c(\"HMC3MPRA_DNA_rep1\", \"HMC3MPRA_DNA_rep2\")))\nrna_count_df <- count_df %>%\n    as.data.frame() %>%\n    set_rownames(.[[\"key_barcode\"]]) %>%\n    select(all_of(c(\"HMC3MPRA_RNA_rep1\", \"HMC3MPRA_RNA_rep2\")))\nif (!all(row.names(dna_count_df) == row.names(rna_count_df))) {\n    stop(\"the row names of DNA are not identical to the row names of RNA\")\n}\n\ncol_names <- c(\"rep1\", \"rep2\")\nobj <- MpraObject(\n    dnaCounts = as.matrix(dna_count_df),\n    rnaCounts = as.matrix(rna_count_df),\n    dnaAnnot = data.frame(replicate = col_names, row.names = colnames(dna_count_df)),\n    rnaAnnot = data.frame(replicate = col_names, row.names = colnames(rna_count_df)),\n    BPPARAM = MulticoreParam()\n)\nobj <- estimateDepthFactors(obj, lib.factor = \"replicate\", which.lib = \"dna\", depth.estimator = \"uq\")\nobj <- estimateDepthFactors(obj, lib.factor = \"replicate\", which.lib = \"rna\", depth.estimator = \"uq\")\nobj <- analyzeQuantification(obj, dnaDesign = ~replicate, rnaDesign = ~1)\n\nalpha_df <- getAlpha(obj) %>%\n    mutate(\n        barcode = row.names(.),\n        log2Alpha = log2(alpha + 1)\n    )\np_density <- ggplot(alpha_df, aes(log2Alpha)) +\n    geom_density(alpha = 0.1, fill = \"magenta\", color = \"magenta\") +\n    scale_y_continuous(expand = expansion(mult = 0.02)) +\n    scale_x_continuous(expand = expansion(mult = 0.02)) +\n    theme_classic(base_size = 20, base_family = \"Arial\")\nppreview(p_density, file = file.path(output_dir, \"alpha.density.pdf\"))\n\nres_df <- testEmpirical(obj, statistic = alpha_df$alpha)\nres_df <- bind_cols(alpha_df, res_df) %>%\n    mutate(\n        pval.mad.adj = p.adjust(pval.mad, method = \"fdr\"),\n        pval.zscore.adj = p.adjust(pval.zscore, method = \"fdr\")\n    ) %>%\n    arrange(pval.mad.adj) %>%\n    mutate(is_active = if_else(pval.mad.adj < 0.05, TRUE, FALSE))\nvroom_write(res_df, file = file.path(output_dir, \"df.tsv\"))\n```\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}