{
  "hash": "7e40719dff4c0f8677a73918d120e8ba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Cut&Tag analysis pipeline\"\nauthor: \"Rui Yang\"\ndate: \"2024-12-30\"\ndate-modified: last-modified\ncategories: [cut&tag]\nformat:\n  html:\n    toc: true\n    toc-depth: 6\n    toc-location: left\n    fig-align: center\n    number-depth: 6\n    number-sections: true\n    fig-cap-location: bottom\n    fig-format: png\n    lightbox: true\n    tbl-cap-location: top\n    page-layout: full\njupyter: julia-1.10\nexecute:\n  warning: false\n  eval: false\n---\n\n\n\n\n## Introduction\n\nBefore running any of the following steps, you should rename your FASTQ files according to [these rules](https://www.neuroborder.com/Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/#copy-your-raw-fastq-data-to-a-new-directory-and-rename-them).\n\n::: {#bfae500c .cell execution_count=1}\n``` {.julia .cell-code}\nwork_dir = \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\ncd(work_dir)\n```\n:::\n\n\n```{r}\nwork_dir <- \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\nsetwd(work_dir)\n```\n\n\n\n\n## MD5SUM check over raw FASTQ files\n\n::: {#89da9651 .cell execution_count=2}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n```\n:::\n\n\n## FASTQC over raw FASTQ files\n\n::: {#b23015ab .cell execution_count=3}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n```\n:::\n\n\n## Quality trimming over raw FASTQ files\n\n::: {#8a9c8fe4 .cell execution_count=4}\n``` {.julia .cell-code}\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)\n```\n:::\n\n\n## FASTQC over clean FASTQ files\n\n::: {#79a3f202 .cell execution_count=5}\n``` {.julia .cell-code}\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n```\n:::\n\n\n## Build Bowtie2 index\n\n::: {#d97dc5c7 .cell execution_count=6}\n``` {.julia .cell-code}\nusing YRUtils\n\nref_fa = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mm10\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" => \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend\n```\n:::\n\n\n## Align reads with Bowtie2\n\n::: {#98042360 .cell execution_count=7}\n``` {.julia .cell-code}\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mm10\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) > 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c > \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) > 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c > \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 -X 2000 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -> println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n```\n:::\n\n\n## Remove reads unmapped and with low quality\n\nAfter this step, if read duplication rate is very low, you can skip the next step - removing duplicate reads.\n\n::: {#461ccd37 .cell execution_count=8}\n``` {.julia .cell-code}\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend\n```\n:::\n\n\n## Remove duplicate reads\n\n### Add `@RG` line if it is not existed\n\nWhen you mark duplicates using `Picard`, tag `@RG` within the header section in your BAM files is mandatory.\n\nYou can check whether your BAM files contain tag `@RG` line with the following code (no if nothing appears):\n\n\n\n\n```{bash}\nsamtools view -H <your BAM file> | grep '@RG'\n```\n\n\n\n\nCheck whether your BAM files are sorted:\n\n\n\n\n```{bash}\nsamtools view -H <your BAM file> | grep '@HD'\n```\n\n\n\n\nSee [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups) for how read group is defined by GATK and [here](https://en.wikipedia.org/wiki/FASTQ_format) for how to extract some info from FASTQ files.\n\nRead group defined by GATK:\n\n\n\n\n```{bash}\n@RG ID:H0164.2  PL:illumina PU:H0164ALXX140820.2    LB:Solexa-272222    PI:0    DT:2014-08-20T00:00:00-0400 SM:NA12878  CN:BI\n```\n\n\n\n\n- `ID:<flow cell name>.<lane number>`: read group identifier.\n\n- `PU:<flow cell barcode>.<lane number>.<sample barcode>`: platform unit (optional). Flow cell barcode refers to the unique identifier for a particular flow cell. Sample barcode is a sample/library-specific identifier.\n\n- `SM:<sample name>`: sample.\n\n- `PL:<platform name>`: platform/technology used to produce the read, such as ILLUMINA, SOLID, LS454, HELICOS and PACBIO.\n\n- `LB:<library name>`: DNA preparation library identifier. This can be used to identify duplicates derived from library preparation.\n\n::: {#cae06323 .cell execution_count=9}\n``` {.julia .cell-code}\n# Construct tag @RG line by extracting some info from paired-end FASTQ files and their file names\n# Only compatible with Casava 1.8 format\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nadd_rg_bam_dir = \"add_rg_bam\"\nplatform = \"ILLUMINA\"\nsamtools_n_threads = 40\n\nmkpath(add_rg_bam_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nfastq_header_names = [\"instrument_name\", \"run_id\", \"flowcell_id\", \"flowcell_lane\", \"lane_tile_number\", \"tile_cluster_x_coord\",\n    \"tile_cluster_y_coord\", \"pair_member_number\", \"is_passed\", \"control_bits_on\", \"index_sequence\"]\nfor raw_fastq_file in raw_fastq_files\n    input_bam_file = joinpath(high_qual_bam_dir, replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" => \".chr_srt.bam\"))\n    output_bam_file = joinpath(add_rg_bam_dir, basename(input_bam_file))\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", raw_fastq_file, \" | grep -P '^@' | head -n 1\")]))\n    fastq_header_line = split(strip(strip(read(cmd, String)), '@'), r\" +\")\n    if length(fastq_header_line) == 2\n        fastq_header_values = YRUtils.BaseUtils.flatten_array(split.(fastq_header_line, \":\"))\n        if length(fastq_header_values) == length(fastq_header_names)\n            fastq_header_dict = Dict(zip(fastq_header_names, fastq_header_values))\n            rg_line = string(\"@RG\\\\tID:\", fastq_header_dict[\"flowcell_id\"], \".\", fastq_header_dict[\"flowcell_lane\"],\n                \"\\\\tPL:\", platform, \"\\\\tSM:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" => \"\"),\n                \"\\\\tLB:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" => \"\"))\n            if isfile(input_bam_file) && !isfile(output_bam_file)\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"samtools addreplacerg -@ \", samtools_n_threads, \" -r '\", rg_line, \"' -o \",\n                        output_bam_file, \" \", input_bam_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                @info string(\"input BAM file (\", input_bam_file, \") is invalid or output BAM file (\", output_bam_file, \") has already existed, you can add @RG line yourself: \", rg_line)\n            end\n        else\n            @error \"unsupported FASTQ header format\"\n        end\n    else\n        @error \"unsupported FASTQ header format\"\n    end\nend\n```\n:::\n\n\n### Remove duplicate reads\n\n::: {#ea289196 .cell execution_count=10}\n``` {.julia .cell-code}\nusing YRUtils\n\nadd_rg_bam_dir = \"add_rg_bam\"\nmark_dup_bam_dir = \"mark_dup_bam\"\nrm_dup_bam_dir = \"rm_dup_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\npicard_path = \"/data/softwares/picard_v3.3.0/picard.jar\"\nsamtools_n_threads = 40\n\nmkpath(mark_dup_bam_dir)\nmkpath(rm_dup_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(add_rg_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    mark_dup_bam_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".mark_dup.bam\"))\n    mark_dup_metrics_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".metrics.txt\"))\n    rm_dup_bam_file = joinpath(rm_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".rm_dup.bam\"))\n\n    cmd = pipeline(\n        Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"java -jar \", picard_path, \" MarkDuplicates --INPUT \", bam_file, \" --OUTPUT \", mark_dup_bam_file,\n                    \" --METRICS_FILE \", mark_dup_metrics_file, \" --VALIDATION_STRINGENCY LENIENT \",\n                    \" --USE_JDK_DEFLATER true --USE_JDK_INFLATER true --ASSUME_SORT_ORDER coordinate \",\n                    \" --REMOVE_DUPLICATES false --TMP_DIR \", tmp_dir)]),\n        );\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\n\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" -F 1804 -f 2 -b \",\n                mark_dup_bam_file, \" -o \", rm_dup_bam_file)]));\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -> println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\nend\n```\n:::\n\n\n## Assess fragment length distributions\n\n### Extract fragment lengths\n\n::: {#0d9a1ff9 .cell execution_count=11}\n``` {.julia .cell-code}\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nfrag_len_dir = \"frag_len\"\nsamtools_n_threads = 40\n\nmkpath(frag_len_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    frag_len_stat_file = joinpath(frag_len_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".frag_len_stat.tsv\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" \", bam_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' 'function abs(x) {return ((x < 0.0) ? -x : x)} {print abs($9)}' \",\n                raw\" | sort -n | uniq -c | awk -v OFS='\\t' '{print $2,$1/2}' > \", frag_len_stat_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n```\n:::\n\n\n### Visualize fragment length distibutions\n\n\n\n\n```{r}\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(vroom)\n\nfrag_len_dir <- \"frag_len\"\n\nfrag_len_stat_files <- list.files(frag_len_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\ndf <- tibble()\nfor (frag_len_stat_file in frag_len_stat_files) {\n    tmp_df <- vroom(frag_len_stat_file, col_names = c(\"frag_len\", \"count\")) %>%\n        mutate(sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(frag_len_stat_file)))\n    df <- bind_rows(df, tmp_df)\n}\ndf <- df %>% arrange(sample, frag_len, count)\n\np <- ggplot(df, aes(frag_len, count, color = sample)) +\n    geom_freqpoly(stat = \"identity\", linewidth = 0.5) +\n    labs(\n        x = \"Fragment length\",\n        y = \"Count\", color = \"Sample\"\n    ) +\n    theme_classic(base_family = \"Arial\", base_size = 20)\nppreview(p, file = file.path(frag_len_dir, \"frag_len_dist.freqpoly.pdf\"))\n```\n\n\n\n\n## Assess the reproducibility of replicates\n\n### Convert BAM into BED\n\n::: {#bcfb2b6b .cell execution_count=12}\n``` {.julia .cell-code}\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bed_dir = \"bam2bed\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam2bed_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".name_srt.bam\"))\n    bed_file = joinpath(bam2bed_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".bed\"))\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" -o \", tmp_name_srt_bam_file, \" \", bam_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"bedtools bamtobed -bedpe -i \", tmp_name_srt_bam_file,\n            raw\" | awk -v FS='\\t' -v OFS='\\t' '$1 == $4 && $6 - $2 < 1000 {print $0}' \",\n            raw\" | cut -f 1,2,6 | sort -k1,1 -k2,2n -k3,3n > \", bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm(tmp_name_srt_bam_file)\nend\n```\n:::\n\n\n### Aggregate fragments into bin counts\n\n::: {#cd7fcf3a .cell execution_count=13}\n``` {.julia .cell-code}\n# We use the middle point of each fragment to infer which bin this fragment belongs to.\n# Calculate stategy:\n# w - window size (e.g. 500)\n# $1 - seqname\n# $2 - start\n# $3 - end\n# int() - round down\n# int(($2 + $3)/(2*w))*w + w/2\n# e.g. all middle points belonging to the left-closed interval [1000, 1500) will have the same quotient 2 by dividing 500,\n# and then, by multiplying 500, and then, by adding 500/2, and finally, the quantity is 1250.\n# Finally we use the vaule 1250 to represent all middle points belonging to [1000, 1500),\n# and then we just need to count the number of 1250 to know how many fragments are enriched in the interval [1000, 1500)\n\nusing YRUtils\n\nbam2bed_dir = \"bam2bed\"\nbin_width = 500\n\nbed_files = YRUtils.BaseUtils.list_files(bam2bed_dir, r\"\\.bed$\", recursive=false, full_name=true)\nfor bed_file in bed_files\n    frag_bin_count_file = joinpath(bam2bed_dir, replace(basename(bed_file), r\"\\.bed$\" => string(\".bin\", bin_width, \".tsv\")))\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", bed_file, \" | awk -v w=\", bin_width,\n                raw\" -v FS='\\t' -v OFS='\\t' '{print $1, int(($2 + $3)/(2*w))*w + w/2}' \",\n                raw\" | sort -k1,1V -k2,2n | uniq -c | awk -v OFS='\\t' '{print $2,$3,$1}' \",\n                \" | sort -k1,1V -k2,2n > \", frag_bin_count_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n```\n:::\n\n\n### Assess the reproducibility of replicates\n\n\n\n\n```{r}\n# At this step, we convert raw counts to Counts Per Million (CPM) to normalize the sequencing depth\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(psych)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggprism)\nlibrary(ggalign)\nlibrary(YRUtils)\n\nbam2bed_dir <- \"bam2bed\"\nqc_dir <- \"qc\"\nbin_width <- 500\ncpm_th <- 1\n\ndir.create(qc_dir, showWarnings = FALSE, recursive = FALSE)\nfiles <- list.files(bam2bed_dir, pattern = paste0(\"\\\\.bin\", bin_width, \"\\\\.tsv$\"), full.names = TRUE, recursive = FALSE)\ndf <- tibble()\nfor (file in files) {\n    tmp_df <- vroom(file, col_names = c(\"seqname\", \"mid_point\", \"count\")) %>%\n        mutate(\n            id = paste0(seqname, \":\", mid_point),\n            sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(file))\n        ) %>%\n        select(id, count, sample)\n    df <- bind_rows(df, tmp_df)\n}\ndf <- df %>%\n    group_by(sample) %>%\n    mutate(cpm = count / sum(count) * 1e6) %>%\n    pivot_wider(id_cols = \"id\", names_from = \"sample\", values_from = \"cpm\", values_fill = 0) %>%\n    select(-all_of(\"id\"))\ndf <- log2(df[rowSums(df >= cpm_th) >= 1, ] + 1)\n\n# Correlation\ncor_res <- corr.test(df, use = \"pairwise\", method = \"pearson\", adjust = \"BH\")\n\np <- ggheatmap(\n    cor_res$r,\n    width = ncol(cor_res$r) * unit(10, \"mm\"),\n    height = nrow(cor_res$r) * unit(10, \"mm\")\n) +\n    scheme_align(free_spaces = \"t\") +\n    scale_fill_gradient2(\n        low = \"blue\", mid = \"white\", high = \"red\",\n        midpoint = 0, limits = c(-1, 1),\n        breaks = c(-1, -0.5, 0, 0.5, 1)\n    ) +\n    labs(fill = \"R\") +\n    guides(x = guide_axis(angle = 45)) +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    anno_top() +\n    ggalign(\n        data = gsub(\"_rep\\\\d+$\", \"\", colnames(cor_res$r)),\n        size = unit(4, \"mm\")\n    ) +\n    geom_tile(aes(y = 1, fill = factor(value))) +\n    scale_y_continuous(breaks = NULL, name = NULL, expand = expansion(0)) +\n    labs(fill = \"Sample\") +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    with_quad(scheme_align(guides = \"t\"), NULL)\nppreview(p, file = file.path(qc_dir, paste0(\"correlation.bin\", bin_width, \".pdf\")))\n\n# PCA\npca <- PCA(t(df), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord <- as.data.frame(pca$ind$coord)\npca_coord$sample <- row.names(pca_coord)\npca_coord$group <- factor(gsub(\"_rep\\\\d+$\", \"\", pca_coord$sample))\npca_eig <- as.data.frame(pca$eig)\n\np <- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 4) +\n    xlab(paste0(\"PC1 (\", round(pca_eig[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(pca_eig[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25) +\n    theme_prism(base_family = \"Arial\", border = TRUE, base_size = 20)\nppreview(p, file = file.path(qc_dir, paste0(\"pca.bin\", bin_width, \".pdf\")))\n```\n\n\n\n\n## Plot signal enrichment profile\n\nIf controls are far more enriched than treatments, you may need to call peaks of treatments without using controls.\n\nYou can try to call peaks of controls and to explore the reason why these regions are enriched (maybe you should remove these regions from peaks of treatments if needed).\n\n::: {#b3a7da50 .cell execution_count=14}\n``` {.julia .cell-code}\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nqc_dir = \"qc\"\nsambamba_n_threads = 40\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 500\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"plotFingerprint -b \", join(bam_files, \" \"),\n            \" --labels \", join(replace.(basename.(bam_files), r\"\\.\\w+\\.bam$\" => \"\"), \" \"),\n            \" --outQualityMetrics \", joinpath(qc_dir, \"plotfingerprint_quality_metrics.log\"),\n            \" --skipZeros --minMappingQuality \", map_qual,\n            \" --numberOfProcessors \", deeptools_n_threads,\n            \" --binSize \", bin_size,\n            \" --plotFile \", joinpath(qc_dir, \"plotfingerprint.pdf\"),\n            \" --outRawCounts \", joinpath(qc_dir, \"plotfingerprint.tsv\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n```\n:::\n\n\n## Convert BAM into BigWig\n\n::: {#5fb81ba4 .cell execution_count=15}\n``` {.julia .cell-code}\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bw_dir = \"bam2bw\"\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 10\nnorm_method = \"RPKM\"\neffective_genome_size = 3372855573\n\nmkpath(bam2bw_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    bw_file = joinpath(bam2bw_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" => \".bw\"))\n    if !isfile(bw_file)\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"bamCoverage -b \", bam_file, \" -o \", bw_file,\n                    \" --numberOfProcessors \", deeptools_n_threads,\n                    \" --binSize \", bin_size,\n                    \" --normalizeUsing \", norm_method,\n                    \" --effectiveGenomeSize \", effective_genome_size,\n                    \" --minMappingQuality \", map_qual,\n                    \" --extendReads\")]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend\n```\n:::\n\n\n## Plot TSS enrichment profile\n\n### Extract transcription start sites\n\n\n\n\n```{r}\nlibrary(tidyverse)\nlibrary(rtracklayer)\nlibrary(vroom)\n\ngff_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\ngff_format <- \"gtf\"\nseq_type <- \"transcript\"\nid_col <- \"transcript_id\"\n\ngff <- import(gff_file, format = gff_format) %>%\n    as.data.frame() %>%\n    as_tibble()\ndf <- gff %>%\n    filter(type == seq_type) %>%\n    mutate(\n        tss_start = if_else(strand == \"-\", end, start - 1),\n        tss_end = if_else(strand == \"-\", end + 1, start),\n        score = 0\n    ) %>%\n    select(all_of(c(\"seqnames\", \"tss_start\", \"tss_end\", id_col, \"score\", \"strand\"))) %>%\n    distinct()\nvroom_write(df,\n    file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".transcription_start_sites.bed\", gff_file),\n    col_names = FALSE, append = FALSE\n)\n```\n\n\n\n\n### Plot TSS enrichment profile\n\n::: {#6178d2dd .cell execution_count=16}\n``` {.julia .cell-code}\nusing YRUtils\n\nbam2bw_dir = \"bam2bw\"\ntss_file = \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.transcription_start_sites.bed\"\nqc_dir = \"qc\"\ndeeptools_n_threads = 40\nbin_size = 10\nbefore_length = 2000\nafter_length = 2000\n\nbw_files = YRUtils.BaseUtils.list_files(bam2bw_dir, r\"\\.bw$\", recursive=false, full_name=true)\nfor bw_file in bw_files\n    tss_cov_mat_file = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" => \".mat.txt\"))\n    tss_cov_mat_gz = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" => \".mat.txt.gz\"))\n    tss_cov_heatmap_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" => \".heatmap.pdf\"))\n    tss_cov_profile_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" => \".profile.pdf\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"computeMatrix reference-point --referencePoint TSS --numberOfProcessors \", deeptools_n_threads,\n                \" -S \", bw_file, \" -R \", tss_file, \" --binSize \", bin_size,\n                \" --beforeRegionStartLength \", before_length, \" --afterRegionStartLength \", after_length,\n                \" --skipZeros --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" => \"\"),\n                \" --outFileNameMatrix \", tss_cov_mat_file, \" --outFileName \", tss_cov_mat_gz)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotHeatmap -m \", tss_cov_mat_gz, \" -o \", tss_cov_heatmap_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" => \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotProfile -m \", tss_cov_mat_gz, \" -o \", tss_cov_profile_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" => \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n```\n:::\n\n\n## Call peaks with MACS\n\n### Pairwise merging of biological replicates\n\ne.g. if you have three biological replicates for a sample, you will get three pooled BAM files: `rep1_vs_rep2`, `rep1_vs_rep3`, and `rep2_vs_rep3`.\n\n::: {#83423361 .cell execution_count=17}\n``` {.julia .cell-code}\nusing YRUtils\nusing Combinatorics\nusing NaturalSort\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nsamtools_n_threads = 40\nsambamba_n_threads = 40\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\ndict = Dict{String,Vector{String}}()\nfor bam_file in bam_files\n    sample = replace(basename(bam_file), r\"_rep\\d+\\.\\w+\\.bam$\" => \"\")\n    if !haskey(dict, sample)\n        dict[sample] = [bam_file]\n    else\n        push!(dict[sample], bam_file)\n    end\nend\nfor sample in keys(dict)\n    combns = collect(combinations(dict[sample], 2))\n    for combn in combns\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"samtools merge -t \", samtools_n_threads,\n                    \" -o \", joinpath(rm_dup_bam_dir,\n                        string(join(sort(replace.(basename.(combn), r\"\\.\\w+\\.bam$\" => \"\"), lt=natural), \"_vs_\"),\n                            \".rm_dup.bam\")),\n                    \" \", join(combn, \" \"))]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\n```\n:::\n\n\n### Call peaks with MACS\n\nIf you want to perform IDR analysis, then use a relax threshold (e.g. `p-value = 0.01`) is suitable.\n\nIf you only want to perform naive overlapping, then use a stringent threshold (e.g. `q-value = 0.05`) is suitable.\n\n#### Call peaks with controls (using p-value)\n\n::: {#a2441901 .cell execution_count=18}\n``` {.julia .cell-code}\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" => \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_<rank>\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2<0) $2=0; if ($3<0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' > \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" > \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc > \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n```\n:::\n\n\n#### Call peaks with controls (using q-value)\n\n::: {#cb10febd .cell execution_count=19}\n``` {.julia .cell-code}\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" => \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_<rank>\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2<0) $2=0; if ($3<0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' > \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" > \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc > \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n```\n:::\n\n\n#### Call peaks without controls (using p-value)\n\n::: {#dc65df87 .cell execution_count=20}\n``` {.julia .cell-code}\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" => \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_<rank>\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2<0) $2=0; if ($3<0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' > \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" > \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc > \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n```\n:::\n\n\n#### Call peaks without controls (using q-value)\n\n::: {#43debf8c .cell execution_count=21}\n``` {.julia .cell-code}\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" => \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_<rank>\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2<0) $2=0; if ($3<0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' > \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" > \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc > \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n```\n:::\n\n\n## Naive overlapping\n\n::: {#5a254e9d .cell execution_count=22}\n``` {.julia .cell-code}\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_peaks\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n]\ninput_dir = \"peak_without_ctl_qval0.05\"\noutput_dir = \"naive_overlap_without_ctl_qval0.05\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools intersect -wo \",\n                \" -a \", joinpath(input_dir, narrow_peak[1]),\n                \" -b \", joinpath(input_dir, narrow_peak[2]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 >= 0.5) || ($21/s2 >= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq \",\n                \" | bedtools intersect -wo \",\n                \" -a stdin -b \", joinpath(input_dir, narrow_peak[3]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 >= 0.5) || ($21/s2 >= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq | pigz -nc > \", joinpath(output_dir, narrow_peak[4]))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n```\n:::\n\n\n## IDR analysis\n\n::: {#4c988128 .cell execution_count=23}\n``` {.julia .cell-code}\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_prefix\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01\",\n    ],\n]\ninput_dir = \"peak_without_ctl_pval0.01\"\noutput_dir = \"idr_without_ctl_pval0.01\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nidr_threshold = 0.05\npeak_type = \"narrowPeak\"\nrank = \"p.value\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    peak1_file = joinpath(input_dir, narrow_peak[2])\n    peak2_file = joinpath(input_dir, narrow_peak[3])\n    pooled_peak_file = joinpath(input_dir, narrow_peak[1])\n\n    prefix = joinpath(output_dir, string(narrow_peak[4], \".idr\", idr_threshold))\n    idr_peak_file = string(prefix, \".\", peak_type, \".gz\")\n    idr_log_file = string(prefix, \".log\")\n    idr_12col_bed_file = string(prefix, \".\", peak_type, \".12-col.bed.gz\")\n    idr_out_file = string(prefix, \".unthresholded-peaks.txt\")\n    idr_tmp_file = string(prefix, \".unthresholded-peaks.txt.tmp\")\n    idr_out_gz_file = string(prefix, \".unthresholded-peaks.txt.gz\")\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"idr \",\n                \" --samples \", peak1_file, \" \", peak2_file,\n                \" --peak-list \", pooled_peak_file,\n                \" --input-file-type \", peak_type,\n                \" --output-file \", idr_out_file,\n                \"  --rank \", rank,\n                \" --soft-idr-threshold \", idr_threshold,\n                \" --log-output-file \", idr_log_file,\n                \" --plot --use-best-multisummit-IDR\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", idr_out_file, \" \", chrsz, \" \", idr_tmp_file, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    if rank == \"signal.value\"\n        col = 7\n    elseif rank == \"p.value\"\n        col = 8\n    elseif rank == \"q.value\"\n        return 9\n    else\n        @error \"invalid score ranking method\"\n    end\n    minus_log10_threshold = -log10(idr_threshold)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '$12>=\", minus_log10_threshold,\n                raw\" {if ($2<0) $2=0; print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' \",\n                \" | sort -k1,1 -k2,2n | uniq | sort -grk\", col, \",\", col,\n                \" | pigz -nc > \", idr_12col_bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"zcat \", idr_12col_bed_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10}' \",\n                \" | pigz -nc > \", idr_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                \" | pigz -nc > \", idr_out_gz_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([idr_out_file, idr_tmp_file, idr_12col_bed_file, string(idr_out_file, \".noalternatesummitpeaks.png\")])\nend\n```\n:::\n\n\n## Pairwise overlapping statistics\n\n\n\n\n```{r}\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggprism)\nlibrary(YRUtils)\nlibrary(patchwork)\n\n# [\"peaks\", \"peaks\", ...]\nnarrow_peaks <- c(\n    \"Satb2_rep1_vs_Satb2_rep2.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep1_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep2_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\"\n)\ninput_dir <- \"naive_overlap_without_ctl_pval0.01\"\noverlap_stat_dir <- \"overlap_stat\"\n\ndir.create(overlap_stat_dir)\ncombns <- combn(narrow_peaks, 2)\nfor (i in seq_len(ncol(combns))) {\n    narrow_peak_pair <- file.path(input_dir, combns[, i])\n    raw_df1 <- vroom(gzfile(narrow_peak_pair[1]), col_names = FALSE) %>%\n        select(X1, X2, X3) %>%\n        arrange(X1, X2, X3) %>%\n        distinct()\n    raw_df2 <- vroom(gzfile(narrow_peak_pair[2]), col_names = FALSE) %>%\n        select(X1, X2, X3) %>%\n        arrange(X1, X2, X3) %>%\n        distinct()\n    overlapped_df <- bt.intersect(\n        a = raw_df1,\n        b = raw_df2,\n        wo = TRUE\n    )\n    overlapped_df1 <- overlapped_df %>%\n        select(V1, V2, V3, V7) %>%\n        arrange(V1, V2, V3) %>%\n        distinct() %>%\n        mutate(\n            percent = V7 / (V3 - V2),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n    overlapped_df2 <- overlapped_df %>%\n        select(V4, V5, V6, V7) %>%\n        arrange(V4, V5, V6) %>%\n        distinct() %>%\n        mutate(\n            percent = V7 / (V6 - V5),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n\n    qc_metrics <- paste0(\n        \">>> \", paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \" vs. \"), \": \\n\",\n        \">> \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[1])), \": \\n\",\n        \"> The number of peaks in total: \", nrow(raw_df1), \"\\n\",\n        \"> The number of peaks overlapped: \", nrow(overlapped_df1), \"\\n\",\n        \"> Percentage: \", round(nrow(overlapped_df1) / nrow(raw_df1), digits = 4), \"\\n\",\n        \">> \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[2])), \": \\n\",\n        \"> The number of peaks in total: \", nrow(raw_df2), \"\\n\",\n        \"> The number of peaks overlapped: \", nrow(overlapped_df2), \"\\n\",\n        \"> Percentage: \", round(nrow(overlapped_df2) / nrow(raw_df2), digits = 4)\n    )\n\n    vroom_write_lines(qc_metrics,\n        file = file.path(\n            overlap_stat_dir,\n            paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".txt\")\n        ),\n        append = FALSE\n    )\n\n    p1 <- ggplot(\n        overlapped_df1 %>%\n            group_by(interval) %>%\n            reframe(n = n()) %>%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"1 vs. 2\") +\n        theme_prism()\n\n    p2 <- ggplot(\n        overlapped_df2 %>%\n            group_by(interval) %>%\n            reframe(n = n()) %>%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"2 vs. 1\") +\n        theme_prism()\n\n    ppreview(p1 | p2, file = file.path(\n        overlap_stat_dir,\n        paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".pdf\")\n    ))\n}\n\n```\n\n\n\n\n## Generate signal tracks with MACS\n\n::: {#a0fd7213 .cell execution_count=24}\n``` {.julia .cell-code}\nusing YRUtils\n\nbdg_dir = \"peak_without_ctl_qval0.05\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n\nbdg_files = YRUtils.BaseUtils.list_files(bdg_dir, r\"\\.bdg$\", recursive=false, full_name=true)\nsamples = unique(replace.(basename.(bdg_files), r\"(_control_lambda|_treat_pileup).bdg$\" => \"\"))\nfor sample in samples\n    bdg_prefix = joinpath(bdg_dir, sample)\n    treat_bdg_file = string(bdg_prefix, \"_treat_pileup.bdg\")\n    ctl_bdg_file = string(bdg_prefix, \"_control_lambda.bdg\")\n    fc_bdg_file = string(bdg_prefix, \".fc.signal.bdg\")\n    fc_srt_bdg_file = string(bdg_prefix, \".fc.signal.srt.bdg\")\n    fc_bw_file = string(bdg_prefix, \".fc.signal.bw\")\n\n    # For fold enrichment signal tracks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 bdgcmp \",\n                \" -t \", treat_bdg_file,\n                \" -c \", ctl_bdg_file,\n                \" --o-prefix \", bdg_prefix,\n                \" -m FE\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools slop -i \", string(bdg_prefix, \"_FE.bdg\"), \" -g \", chrsz, \" -b 0 \",\n                \" | bedClip stdin \", chrsz, \" \", fc_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort and remove any overlapping regions in bedgraph by comparing two lines in a row\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k1,1 -k2,2n \", fc_bdg_file,\n                raw\" | awk -v OFS='\\t' '{if (NR==1 || NR>1 && (prev_chr!=$1 || prev_chr==$1 && prev_chr_e<=$2)) {print $0}; prev_chr=$1; prev_chr_e=$3;}' > \", fc_srt_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedGraphToBigWig \", fc_srt_bdg_file, \" \", chrsz, \" \", fc_bw_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([fc_bdg_file, fc_srt_bdg_file, treat_bdg_file, ctl_bdg_file, string(bdg_prefix, \"_FE.bdg\")])\nend\n```\n:::\n\n\n## Extending peaks on both sides from summits\n\n\n\n\n```{r}\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nextend_width <- 250\nmerge_dist <- 10\nchrsz_file <- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\npeak_files <- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\"\n)\n\nchrsz <- vroom(chrsz_file, col_names = FALSE)\nfor (peak_file in peak_files) {\n    tmp_df <- vroom(gzfile(peak_file), col_names = FALSE) %>%\n        mutate(\n            summit_seqname = X1,\n            summit_start = X2 + X10,\n            summit_end = summit_start,\n        ) %>%\n        select(summit_seqname, summit_start, summit_end) %>%\n        arrange(summit_seqname, summit_start, summit_end) %>%\n        distinct()\n    df <- bt.slop(i = tmp_df, g = chrsz, b = extend_width) %>%\n        distinct() %>%\n        arrange(V1, V2, V3)\n    df <- bt.merge(df, d = merge_dist) %>%\n        mutate(\n            V4 = paste0(V1, \":\", V2, \"-\", V3),\n            V5 = 1000,\n            V6 = \"+\"\n        )\n    vroom_write(df,\n        file = gsub(\"\\\\.narrowPeak\\\\.gz$\", paste0(\".summits.b\", 2 * extend_width, \".bed\"), peak_file),\n        col_names = FALSE,\n        append = FALSE\n    )\n}\n```\n\n\n\n\n## Generate final peak sets\n\n**Scheme 1: (recommended)**\n\n1. Call peaks with MACS using `p-value = 0.05`\n\n2. Perform pairwise IDR analysis using `idr threshold = 0.05`\n\n3. Merge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\n**Scheme 2:**\n\n1. Call peaks with MACS using `q-value = 0.05`\n\n2. Perform pairwise naive overlapping\n\n3. Merge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\n\n\n\n```{r}\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nmerge_dist <- 10\noutput_dir <- \"final_peak\"\npeak_files <- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\"\n)\noutput_file <- \"Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\n\ndir.create(output_dir)\ndf <- tibble()\nfor (peak_file in peak_files) {\n    df <- bind_rows(\n        df,\n        vroom(gzfile(peak_file), col_names = FALSE) %>%\n            select(X1, X2, X3)\n    )\n}\ndf <- distinct(df) %>%\n    arrange(X1, X2, X3)\ndf <- bt.merge(df, d = merge_dist) %>%\n    mutate(\n        V4 = paste0(V1, \":\", V2, \"-\", V3),\n        V5 = 1000,\n        V6 = \"+\"\n    )\nvroom_write(df, file = file.path(output_dir, output_file), col_names = FALSE, append = FALSE)\n```\n\n\n\n\n## De novo motif finding with Homer\n\n### De novo motif finding with Homer\n\n::: {#d6e1fe2d .cell execution_count=25}\n``` {.julia .cell-code}\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/homer/de_novo/narrow_peak\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" => \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' > \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" => \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" \", extra_args, \" \", if !isempty(known_motifs_file)\n                string(\" -mknown \", known_motifs_file)\n            else\n                \"\"\n            end)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n```\n:::\n\n\n### Finding instances of specific motifs\n\n::: {#394a6f7b .cell execution_count=26}\n``` {.julia .cell-code}\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\noutput_file = \"motif_instances.txt\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" => \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' > \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" => \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" -find \", known_motifs_file, \" \", extra_args, \" > \", joinpath(output_dir, output_file))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n```\n:::\n\n\n## Peak annotation\n\n### Make TxDb object from GFF3/GTF file\n\n\n\n\n```{r}\nlibrary(txdbmaker)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(GenomeInfoDb)\n\nanno_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\nchrsz_file <- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nanno_format <- \"gtf\"\norganism <- \"Mus musculus\"\n# Sugar glider (Petaurus breviceps): 34899\n# Mouse (Mus musculus): 10090\ntaxonomy_id <- 10090\ngenome <- \"mm10\"\ncircular_chrs <- c(\"chrM\")\n\nchrsz <- vroom(chrsz_file, col_names = FALSE)\nseqinfo <- Seqinfo(\n    seqnames = chrsz$X1,\n    seqlengths = chrsz$X2,\n    isCircular = if_else(chrsz$X1 %in% circular_chrs, TRUE, FALSE),\n    genome = genome\n)\ntxdb <- makeTxDbFromGFF(\n    file = anno_file,\n    format = anno_format,\n    organism = organism,\n    taxonomyId = taxonomy_id,\n    chrominfo = seqinfo\n)\n# loadDB()\nsaveDb(txdb, file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".TxDb.sqlite\", anno_file))\n```\n\n\n\n\n### Profile of peaks binding to TSS/body/TTS regions\n\n\n\n\n```{r}\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggprism)\nlibrary(ggplot2)\n\npeak_file <- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir <- \"final_peak/peak_anno/narrow_peak\"\nupstream_dist <- 2000\ndownstream_dist <- 2000\nby_what <- \"gene\"\ntypes <- c(\"start_site\", \"body\", \"end_site\")\nnbin <- 400\n\npeak <- readPeakFile(peak_file)\ntxdb <- loadDb(txdb_file)\nfor (type in types) {\n    profile_p <- plotPeakProf2(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme_prism(\n        base_size = 14,\n        base_family = \"Arial\",\n        border = TRUE\n    )\n\n    profile_p_file <- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".profile.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(profile_p, file = profile_p_file)\n\n    heatmap_p <- peakHeatmap(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme(\n        text = element_text(family = \"Arial\", size = 14)\n    )\n\n    heatmap_p_file <- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".heatmap.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(heatmap_p, file = heatmap_p_file)\n}\n```\n\n\n\n\n### Peak annotation\n\n\n\n\n```{r}\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggplot2)\nlibrary(ggprism)\nlibrary(vroom)\n\npeak_file <- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir <- \"final_peak/peak_anno/narrow_peak\"\ntss_region <- c(-2000, 2000)\nby_what <- \"transcript\"\n\npeak <- readPeakFile(peak_file)\ntxdb <- loadDb(txdb_file)\npeak_anno <- annotatePeak(\n    peak = peak,\n    tssRegion = tss_region,\n    TxDb = txdb,\n    level = by_what\n)\n\npeak_anno_file <- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno.tsv\"),\n        basename(peak_file)\n    )\n)\nvroom_write(as.data.frame(peak_anno),\n    file = peak_anno_file,\n    col_names = TRUE, append = FALSE\n)\n\npeak_anno_stat_df <- getAnnoStat(peak_anno)\n\npie_p <- ggplot(peak_anno_stat_df, aes(x = \"\", y = Frequency, fill = Feature)) +\n    geom_col() +\n    coord_polar(theta = \"y\") +\n    theme_void(base_size = 20, base_family = \"Arial\")\n\npie_p_file <- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_pie.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(pie_p, file = pie_p_file)\n\ntss_distribution_p <- plotDistToTSS(peak_anno) +\n    labs(title = NULL) +\n    theme_prism(border = TRUE, base_size = 20, base_family = \"Arial\") +\n    theme(legend.title = element_text())\n\ntss_distribution_p_file <- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_tss_distribution.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(tss_distribution_p, file = tss_distribution_p_file)\n```\n\n\n\n\n### Functional enrichment analysis\n\n\n\n\n```{r}\nlibrary(vroom)\nlibrary(magrittr)\nlibrary(clusterProfiler)\nlibrary(AnnotationDbi)\nlibrary(tidyverse)\n\npeak_anno_file <- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/peak_anno/narrow_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.transcript.anno.tsv\"\nmpt_file <- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.trna.ercc.phix.gtf.gz.gene_id_name_mapping_table.tsv\"\norgdb_file <- \"/data/biodatabase/species/mm10/genome/anno/org.Mm.eg.db.sqlite\"\n\nmpt_df <- vroom(mpt_file) %>%\n    select(gene_id, gene_name) %>%\n    distinct()\npeak_anno_df <- vroom(peak_anno_file) %>%\n    inner_join(mpt_df, by = c(\"geneId\" = \"gene_id\")) %>%\n    select(seqnames, start, end, V4, V5, V6, gene_name, distanceToTSS) %>%\n    set_colnames(c(\"seqname\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"gene_name\", \"dist_to_tss\")) %>%\n    distinct()\nvroom_write(peak_anno_df,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.with_names.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)\n\norgdb <- loadDb(orgdb_file)\nego <- enrichGO(\n    unique(na.omit(peak_anno_df[[\"gene_name\"]])),\n    OrgDb = orgdb,\n    keyType = \"SYMBOL\",\n    ont = \"ALL\",\n    pvalueCutoff = 0.05,\n    qvalueCutoff = 0.05,\n    pAdjustMethod = \"BH\",\n    minGSSize = 10,\n    maxGSSize = 1000,\n    readable = FALSE,\n    pool = FALSE\n)\nego\nnrow(ego@result)\nsaveRDS(ego, file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.rds\", peak_anno_file))\nvroom_write(ego@result,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)\n```\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}