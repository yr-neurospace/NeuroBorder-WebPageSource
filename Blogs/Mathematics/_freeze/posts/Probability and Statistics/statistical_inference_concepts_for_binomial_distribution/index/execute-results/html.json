{
  "hash": "4f4a89de0a267205b38a24c3ae6bef2e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Statistical inference concepts for binomial distribution\"\nauthor: \"Rui Yang\"\ndate: \"2024-08-31\"\ndate-modified: last-modified\ncategories: [Probability and Statistics, Statistical inference, Binomial distribution]\nformat:\n  html:\n    toc: true\n    toc-depth: 6\n    toc-location: left\n    number-depth: 6\n    number-sections: true\n    fig-align: center\n    fig-cap-location: bottom\n    fig-format: png\n    lightbox: true\n    tbl-cap-location: top\n    page-layout: full\njupyter: julia-1.10\n---\n\n\n## Bernoulli distribution\n\nIn an experiment, there are only two outcomes: success $A$ ($X = 1$) and failure $\\overline{A}$ ($X = 0$), and the probability of success is $p$. Such an experiment is called a Bernoulli experiment.\n\nIn a Bernoulli experiment, the number of success, denoted by $X$, is a random variable and its distribution is called a Bernoulli distribution or a two-point distribution.\n\nClearly, the PMF of a Bernoulli distribution is\n\n$$\np(x) = P(X = x) = \\begin{cases}\np &\\text{if } x = 1 \\\\\n1 - p &\\text{if } x = 0\n\\end{cases}\n$$\n\nThen $E(X) = \\sum_{i=1}^{\\infty} x_i p(x_i) = 1 \\times p + 0 \\times (1 - p) = p$, and $Var(X) = E[(X - E(X))^2] = \\sum_{i=1}^{\\infty} (x_i - E(X))^2 p(x_i) = (1 - p)^2 \\times p + (0 - p)^2 \\times (1 - p) = p(1 - p)$.\n\n## Binomial distribution\n\nIn $n$ independent Bernoulli trials, each with the probability of success $p$, the number of success $Y$ is distributed as a binomial distribution.\n\nThe PMF of a binomial distribution is\n\n$$\np(k) = P(Y = k) = \\binom{n}{k} p^k (1-p)^{n - k}, k = 0, 1, ..., n\n$$\n\nClearly, a binomial distribution can be regarded as the sum of $n$ independent Bernoulli distributions, i.e., $Y = X_1 + \\cdots + X_n$. Then, by using the arithmetic properties of expectation and variance, we have $E(Y) = np$ and $Var(Y) = np(1 - p)$.\n\nDue to $Y$ is the sum of $n$ independent Bernoulli random variables, we have $Y\\ \\ \\widetilde{\\text{approx}}\\ \\ N(np, np(1 - p))$ as $n \\to \\infty$ based on the central limit theorem. We also have $\\frac{Y}{n}\\ \\ \\widetilde{\\text{approx}}\\ \\ N(p, \\frac{p(1 - p)}{n})$.\n\n## Point estimations of $p$\n\n### Method of Moments (MM)\n\n$$\n\\begin{align}\n\\hat{m} &= \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\frac{k}{n} \\\\\nm &= p \\\\\n\\hat{m} &= m\n\\end{align}\n$$\n\nHence\n\n$$\np = \\frac{k}{n}\n$$\n\n### Maximum likelihood estimate (MLE)\n\nFor given $n$ and $k$, we need to find a $p$ to maximize the likelihood function:\n\n$$\n\\begin{align}\nL(p) &= \\binom{n}{k} p^k (1 - p)^{n - k} \\\\\n\\log{L(p)} &= \\log{\\binom{n}{k} + k\\log{p} + (n - k)\\log{(1 - p)}} \\\\\n\\frac{d\\log{L(p)}}{dp} &= \\frac{k}{p} - \\frac{n - k}{1 - p} = 0 \\\\\np &= \\frac{k}{n}\n\\end{align}\n$$\n\nThere is a point $p = \\frac{k}{n}$ that maximizes $L(p)$.\n\n## Confidence intervals of $p$\n\n### Normal approximation\n\n\n```{=html}\n<object data=\"Statistical inference concepts for binomial distribution.pdf\" type=\"application/pdf\" width=\"100%\" height=\"100%\" style=\"min-width:100vh;min-height:100vh;\">\n  <p>It appears you don't have a PDF plugin for this browser. No biggie. You can click <a href=\"Statistical inference concepts for binomial distribution.pdf\">here</a> to download the PDF file.</p>\n</object>\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}