[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioinformatics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nBulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, and sva\n\n\n\n\n\n\nbulk rna-seq\n\n\ndifferential expression\n\n\nbatch correction\n\n\ndeseq2\n\n\nlimma\n\n\nsva\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBuild ENCODE ATAC/ChIP-Seq pipeline indices\n\n\n\n\n\n\nencode\n\n\natac-seq\n\n\nchip-seq\n\n\nindex\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBuild ENCODE RNA-Seq pipeline indices\n\n\n\n\n\n\nencode\n\n\nrna-seq\n\n\nindex\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nDE-related analysis for bulk proteomics\n\n\n\n\n\n\nbulk proteomics\n\n\ndifferential expression\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMPRA analysis pipeline\n\n\n\n\n\n\nmpra\n\n\n\n\n\n\n\n\n\nJan 8, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMiscellaneous R code\n\n\n\n\n\n\nr\n\n\nmisc\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCut&Tag analysis pipeline\n\n\n\n\n\n\ncut&tag\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nAgilent mRNA expression microarray analysis using limma\n\n\n\n\n\n\nagilent\n\n\nmrna\n\n\nmicroarray\n\n\nlimma\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify housekeeping genes\n\n\n\n\n\n\nbulk rna-seq\n\n\nhousekeeping genes\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBulk RNA-seq pseudotime analysis\n\n\n\n\n\n\nbulk rna-seq\n\n\npseudotime\n\n\n\n\n\n\n\n\n\nSep 9, 2024\n\n\nRui Yang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html",
    "href": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "",
    "text": "In scRNA-seq, various tools, such as Monocle3, provide the capability of performing pseudotime analysis. In brief, assume that there are both progenitors and more differentiated progenies in an scRNA-seq dataset. If we consider the most undeferentiated progenitors as the developmental origin (assigning them the number \\(0\\)), and the most differentiated progenies the developmental ends (assigning them the number 10), then we can assign each intermediate cell within them a number between \\(0\\) and \\(10\\). For cells with numbers approaching \\(0\\) more, they are more similar with the progenitors in terms of their RNA expression patterns and vice versa. Once we assign each cell a number (i.e. a developmental pseudotime point) and order them based on their pseudotime, we can arrange highly variable genes based on their peaking expression patterns (i.e. genes with peaking expression patterns at early stages are placed at the left, etc.).\nIn bulk RNA-seq, the number of samples is far less than the number of cells in scRNA-seq, where each cell can be regarded as a sample, so the gene expression dynamics along the developmental stages are not so smooth (i.e. jagged) as we have seen in scRNA-seq if we do the same analysis in bulk RNA-seq as in scRNA-seq. Therefore, to make the gene expression dynamics smoother along the developmental stages, we need to obtain more pseudo/interpolated time points than those we have.\nBriefly, to achieve this goal, we need to do the following things:\n\nDefine the time scale among developmental samples based on their mutual Euclidean distances calculated from their coordinates (Dim.1, Dim.2) obtained from their PCA space (i.e. consider the earliest sample as the developmental origin, assign it \\(0\\), and for the remaining samples, use their Euclidean disntances accumulated from the origin as their developmental time points).\nScale the time scale to (0, 10).\nFit a spline for each gene based its \\((time, expression)\\) pairs along the actual developmental stages, and use this fitted spline to interpolate more \\((time, expression)\\) pairs (using the loess method in modelr package).\nFor each gene, obtain its PCA coordinate (Dim.1, Dim.2), and then feed all possible signed combinations of Dim.1 and Dim.2 of all genes to atan2 to get a sequence of values used to sort genes.\nVisualize gene expression dynamics along interpolated time points to pick the expected one."
  },
  {
    "objectID": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html#introduction",
    "href": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html#introduction",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "",
    "text": "In scRNA-seq, various tools, such as Monocle3, provide the capability of performing pseudotime analysis. In brief, assume that there are both progenitors and more differentiated progenies in an scRNA-seq dataset. If we consider the most undeferentiated progenitors as the developmental origin (assigning them the number \\(0\\)), and the most differentiated progenies the developmental ends (assigning them the number 10), then we can assign each intermediate cell within them a number between \\(0\\) and \\(10\\). For cells with numbers approaching \\(0\\) more, they are more similar with the progenitors in terms of their RNA expression patterns and vice versa. Once we assign each cell a number (i.e. a developmental pseudotime point) and order them based on their pseudotime, we can arrange highly variable genes based on their peaking expression patterns (i.e. genes with peaking expression patterns at early stages are placed at the left, etc.).\nIn bulk RNA-seq, the number of samples is far less than the number of cells in scRNA-seq, where each cell can be regarded as a sample, so the gene expression dynamics along the developmental stages are not so smooth (i.e. jagged) as we have seen in scRNA-seq if we do the same analysis in bulk RNA-seq as in scRNA-seq. Therefore, to make the gene expression dynamics smoother along the developmental stages, we need to obtain more pseudo/interpolated time points than those we have.\nBriefly, to achieve this goal, we need to do the following things:\n\nDefine the time scale among developmental samples based on their mutual Euclidean distances calculated from their coordinates (Dim.1, Dim.2) obtained from their PCA space (i.e. consider the earliest sample as the developmental origin, assign it \\(0\\), and for the remaining samples, use their Euclidean disntances accumulated from the origin as their developmental time points).\nScale the time scale to (0, 10).\nFit a spline for each gene based its \\((time, expression)\\) pairs along the actual developmental stages, and use this fitted spline to interpolate more \\((time, expression)\\) pairs (using the loess method in modelr package).\nFor each gene, obtain its PCA coordinate (Dim.1, Dim.2), and then feed all possible signed combinations of Dim.1 and Dim.2 of all genes to atan2 to get a sequence of values used to sort genes.\nVisualize gene expression dynamics along interpolated time points to pick the expected one."
  },
  {
    "objectID": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html#pipeline",
    "href": "posts/bulk RNA-seq/bulk_rna-seq_pseudotime_analysis/index.html#pipeline",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "2 Pipeline",
    "text": "2 Pipeline\n\nsuppressWarnings(suppressMessages(library(vroom)))\nsuppressWarnings(suppressMessages(library(tidyverse)))\nsuppressWarnings(suppressMessages(library(ggplot2)))\nsuppressWarnings(suppressMessages(library(ggrepel)))\nsuppressWarnings(suppressMessages(library(magrittr)))\nsuppressWarnings(suppressMessages(library(FactoMineR)))\nsuppressWarnings(suppressMessages(library(ComplexHeatmap)))\nsuppressWarnings(suppressMessages(library(scales)))\nsuppressWarnings(suppressMessages(library(modelr)))\nsuppressWarnings(suppressMessages(library(RColorBrewer)))\nsuppressWarnings(suppressMessages(library(patchwork)))\nsuppressWarnings(suppressMessages(library(showtext)))\n\n\nfont_family &lt;- \"Arial\"\n\nfont_df &lt;- filter(font_files(), family == font_family)\nfont_add(\n    family = font_family,\n    regular = if (\"Regular\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Regular\"] else stop(\"no font file found\"),\n    bold = if (\"Bold\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Bold\"] else NULL,\n    italic = if (\"Bold Italic\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Bold Italic\"] else NULL,\n    bolditalic = if (\"Italic\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Italic\"] else NULL\n)\nshowtext_auto()\n\n\n# specify input gene expression matrix\n# containing one ID column named \"GeneID\"\n# the remaining columns are sample columns named in the form of \"SampleID.Replicate\" (e.g. Skin.1, Skin.2, etc.)\n# SampleID must not contain \".\"\n# Replicate must be one or more integers\nexpr_file &lt;- \"./data/RNA_TPM.txt\"\n# specify the sample levels, reflecting their actual developmental stages\nsample_dev_order &lt;- c(\"DAI0\", \"DAI3\", \"DAI6\", \"DAI9\", \"DAI12\")\ntime_points_num &lt;- 500\n\n\nexpr &lt;- vroom(expr_file) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"GeneID\"]]) %&gt;%\n    select(-all_of(\"GeneID\")) %&gt;%\n    distinct()\nsample_df &lt;- strsplit(names(expr), \".\", fixed = T) %&gt;%\n    do.call(rbind, .) %&gt;%\n    as.data.frame() %&gt;%\n    set_colnames(c(\"SampleID\", \"Replicate\")) %&gt;%\n    mutate(Sample = paste0(SampleID, \".\", Replicate))\n\n\n# calculate the mean expression value of each gene within each sample\ndata &lt;- data.frame(GeneID = row.names(expr))\nfor (id in unique(sample_df[[\"SampleID\"]])) {\n    id_reps &lt;- filter(sample_df, SampleID == id) %&gt;%\n        pull(Sample) %&gt;%\n        unique()\n    id_mean_expr &lt;- data.frame(Expr = rowMeans(expr[, id_reps]))\n    names(id_mean_expr) &lt;- id\n    data &lt;- bind_cols(data, id_mean_expr)\n}\ndata &lt;- as.data.frame(data) %&gt;%\n    set_rownames(.[[\"GeneID\"]]) %&gt;%\n    select(-all_of(\"GeneID\"))\n\n\n# use row variances to identify the top 3000 most variable genes\n# log2-trsanformation is recommended for reducing variance variation among genes\ndata &lt;- log2(data + 1)\ndata[[\"var\"]] &lt;- apply(data, 1, var)\ndata &lt;- data %&gt;%\n    arrange(desc(var)) %&gt;%\n    slice_head(n = 3000) %&gt;%\n    select(-all_of(\"var\"))\ndata &lt;- data[, sample_dev_order]\n\n\n# perform PCA analysis over samples (samples as observations)\n# calculate Euclidean distances among samples based on their coordinates (Dim.1, Dim.2) in sample PCA space\n# obtain the developmental time scale by accumulating distances of mutual samples\nsample_pca &lt;- PCA(t(data), scale.unit = T, ncp = 5, graph = F)\nsample_pca_coords &lt;- sample_pca$ind$coord[, 1:2]\n\n# visualize sample positions in PCA space\nsample_pca_coords_vis &lt;- as.data.frame(sample_pca_coords)\nsample_pca_coords_vis[[\"Sample\"]] &lt;- row.names(sample_pca_coords_vis)\nsample_pca_eig_vis &lt;- as.data.frame(sample_pca$eig)\n\nggplot(sample_pca_coords_vis, aes(Dim.1, Dim.2)) +\n    geom_point(size = 2) +\n    geom_text_repel(aes(label = Sample), size = 5, min.segment.length = 3) +\n    xlab(paste0(\"PC1 (\", round(sample_pca_eig_vis[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(sample_pca_eig_vis[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.title.x = element_text(size = 26),\n        axis.title.y = element_text(size = 26),\n        axis.text.x = element_text(size = 24),\n        axis.text.y = element_text(size = 24),\n        legend.text = element_text(size = 24),\n        legend.title = element_text(size = 26),\n        text = element_text(family = \"Arial\")\n    )\n\n\n\n\n\n\n\n\n\nsample_dists &lt;- as.matrix(dist(sample_pca_coords, method = \"euclidean\"))\n\n# visualize sample distances via heatmap\nHeatmap(sample_dists, cluster_rows = F, cluster_columns = F)\n\n\n\n\n\n\n\n\n\n# calculate the developmental time scale by accumulating distances of mutual samples along the actual developmental stages\nraw_timeline &lt;- cumsum(c(0, sapply(2:ncol(data), function(x) {\n    sample_dists[x - 1, x]\n})))\n# scale the raw time scale to (0, 10)\nnew_timeline &lt;- scales::rescale(raw_timeline, to = c(0, 10))\n\n\n# fit a spline for each gene and obtain 500 time points by interpolation\ndata_scale &lt;- as.data.frame(t(scale(t(data))))\n\n# interpolate more time points (e.g., 500) to make the expression dynamics smoother along the developmental stages\n# based on the fitted spline for each gene (using the loess method in modelr package)\npseudotime_model_fun &lt;- function(sample_value, sample_timeline, time_points_num = 500) {\n    grid &lt;- data.frame(time = seq(0, 10, length.out = time_points_num))\n    data &lt;- tibble(value = sample_value, time = sample_timeline)\n    model &lt;- loess(value ~ time, data)\n    predict &lt;- add_predictions(grid, model)\n    return(predict)\n}\n\npseudotime_model_res &lt;- apply(data_scale, 1, pseudotime_model_fun, sample_timeline = new_timeline, time_points_num = time_points_num)\nres &lt;- lapply(pseudotime_model_res, function(x) {\n    x[[\"pred\"]]\n}) %&gt;%\n    do.call(rbind, .) %&gt;%\n    as.data.frame() %&gt;%\n    set_colnames(pseudotime_model_res[[1]][[\"time\"]])\n\n\n# perform PCA analysis over genes\n# use atan2 method to sort genes based on their coordinates (Dim.1, Dim.2) in gene PCA space\ngene_pca &lt;- PCA(res, scale.unit = T, ncp = 5, graph = F)\ngene_pca_coords &lt;- gene_pca$ind$coord[, 1:2]\nres &lt;- bind_cols(res, gene_pca_coords)\n\n# we have four signed combinations of Dim.1 and Dim.2\nres[[\"atan2.1\"]] &lt;- atan2(res[[\"Dim.1\"]], res[[\"Dim.2\"]])\nres[[\"atan2.2\"]] &lt;- atan2(res[[\"Dim.1\"]], -res[[\"Dim.2\"]])\nres[[\"atan2.3\"]] &lt;- atan2(-res[[\"Dim.1\"]], res[[\"Dim.2\"]])\nres[[\"atan2.4\"]] &lt;- atan2(-res[[\"Dim.1\"]], -res[[\"Dim.2\"]])\n\n# sort genes based on their atan2 values in ascending order\nres_order1 &lt;- arrange(res, res[[\"atan2.1\"]])\nres_order2 &lt;- arrange(res, res[[\"atan2.2\"]])\nres_order3 &lt;- arrange(res, res[[\"atan2.3\"]])\nres_order4 &lt;- arrange(res, res[[\"atan2.4\"]])\n\n# pick the expected one\np1 &lt;- Heatmap(as.matrix(res_order1[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order1\",\n    heatmap_legend_param = list(title = \"Order1\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np2 &lt;- Heatmap(as.matrix(res_order2[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order2\",\n    heatmap_legend_param = list(title = \"Order2\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np3 &lt;- Heatmap(as.matrix(res_order3[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order3\",\n    heatmap_legend_param = list(title = \"Order3\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np4 &lt;- Heatmap(as.matrix(res_order4[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order4\",\n    heatmap_legend_param = list(title = \"Order4\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\n\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html",
    "title": "Identify housekeeping genes",
    "section": "",
    "text": "In brief, housekeeping genes are those with higher expression levels, low variances, and ubiquitous expression profiles across samples and species.\nIn paper “What are housekeeping genes” by Chintan J. Joshi, housekeeping genes are defined as those with the following four properties:\n\nHigher expression stability\nCellular maintenance\nEssentiality\nConservation"
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#methods-overview",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#methods-overview",
    "title": "Identify housekeeping genes",
    "section": "",
    "text": "In brief, housekeeping genes are those with higher expression levels, low variances, and ubiquitous expression profiles across samples and species.\nIn paper “What are housekeeping genes” by Chintan J. Joshi, housekeeping genes are defined as those with the following four properties:\n\nHigher expression stability\nCellular maintenance\nEssentiality\nConservation"
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-1",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-1",
    "title": "Identify housekeeping genes",
    "section": "2 Method 1",
    "text": "2 Method 1\nFrom “A Comprehensive Mouse Transcriptomic BodyMap across 17 Tissues by RNA-seq” by Bin Li.\nCriteria for identification of housekeeping genes:\n\nHighly expressed in all biological samples (\\(FPKM &gt; 1\\));\nLow variance across tissues: std(log2(FPKM)) &lt; 1;\nNo logarithmic expression value differed from the averaged log2(FPKM) value by a factor of two (i.e. fourfold) or more.\n\nCriteria for identification of reference genes:\n\n\\(FPKM &gt; 50\\) across all biological samples;\nstd(log2(FPKM)) &lt; 0.5 over tissues;\nNo logarithmic expression value differed from the averaged log2(FPKM) value by a factor of one (i.e. twofold) or more."
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-2",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-2",
    "title": "Identify housekeeping genes",
    "section": "3 Method 2",
    "text": "3 Method 2\nFrom “Housekeeping protein‑coding genes interrogated with tissue and individual variations” by Kuo‑FengTung.\nGini coefficient of inequality (Gini index):\n\n\\(TPM &gt; 0.05\\);\n\\(\\text{Gini index} &lt; 0.2\\)."
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-3",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#method-3",
    "title": "Identify housekeeping genes",
    "section": "4 Method 3",
    "text": "4 Method 3\nFrom “The evolution of gene expression levels in mammalian organs” by David Brawand.\nPipeline used to pick housekeeping genes and normalize expression levels across species:\n\nConvert TPM to \\(log2(TPM+1)\\);\nRetrieve and only keep one-to-one orthologous genes across all species with confidence equal to \\(1\\) from Ensembl BioMart;\nSort orthologs based on TPMs in descending order and represent each gene by its TPM rank in each sample;\nCalculate the standard deviation and median of each ortholog based on its ranks across samples;\nKeep orthologs the medians of which are within \\(0.25 \\times \\text{the number of orthologs} \\sim 0.75 \\times \\text{the number of orthologs}\\) (discarding those orthologs with expression levels extremely high or extremely low across samples);\nRetain the \\(1000\\) orthologs with the lower variances (standard deviations);\nCalculate the medians of those \\(1000\\) orthologs’ TPMs in each sample;\nCalculate the scaling factor of each sample: the scaling factor of sample A = TPM median of sample A / median(TPM median of all samples);\nFor each sample, scaled TPM = TPM / scaling factor.\n\nNote: be aware of the fact that the expression difference of each same/homologous gene among species and the difference among batches are different."
  },
  {
    "objectID": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#reference-datasets",
    "href": "posts/bulk RNA-seq/identify_housekeeping_genes/index.html#reference-datasets",
    "title": "Identify housekeeping genes",
    "section": "5 Reference datasets",
    "text": "5 Reference datasets\nHuman housekeeping genes: https://www.gsea-msigdb.org/gsea/msigdb/cards/HOUNKPE_HOUSEKEEPING_GENES.html.\nMouse housekeeping genes: https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HOUNKPE_HOUSEKEEPING_GENES.html."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bioinformatics",
    "section": "",
    "text": "Here is a collection of blogs related to bioinfomatics, such as bulk RNA-seq, scRNA-seq, etc.\nEmail: 2413667864@qq.com"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html",
    "title": "Cut&Tag analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#introduction",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#introduction",
    "title": "Cut&Tag analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#md5sum-check-over-raw-fastq-files",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#md5sum-check-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "2 MD5SUM check over raw FASTQ files",
    "text": "2 MD5SUM check over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-raw-fastq-files",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "3 FASTQC over raw FASTQ files",
    "text": "3 FASTQC over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#quality-trimming-over-raw-fastq-files",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#quality-trimming-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "4 Quality trimming over raw FASTQ files",
    "text": "4 Quality trimming over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-clean-fastq-files",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-clean-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "5 FASTQC over clean FASTQ files",
    "text": "5 FASTQC over clean FASTQ files\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#build-bowtie2-index",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#build-bowtie2-index",
    "title": "Cut&Tag analysis pipeline",
    "section": "6 Build Bowtie2 index",
    "text": "6 Build Bowtie2 index\n\nusing YRUtils\n\nref_fa = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mm10\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#align-reads-with-bowtie2",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#align-reads-with-bowtie2",
    "title": "Cut&Tag analysis pipeline",
    "section": "7 Align reads with Bowtie2",
    "text": "7 Align reads with Bowtie2\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mm10\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) &gt; 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) &gt; 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 -X 2000 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-reads-unmapped-and-with-low-quality",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-reads-unmapped-and-with-low-quality",
    "title": "Cut&Tag analysis pipeline",
    "section": "8 Remove reads unmapped and with low quality",
    "text": "8 Remove reads unmapped and with low quality\nAfter this step, if read duplication rate is very low, you can skip the next step - removing duplicate reads.\n\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-duplicate-reads",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-duplicate-reads",
    "title": "Cut&Tag analysis pipeline",
    "section": "9 Remove duplicate reads",
    "text": "9 Remove duplicate reads\n\n9.1 Add @RG line if it is not existed\nWhen you mark duplicates using Picard, tag @RG within the header section in your BAM files is mandatory.\nYou can check whether your BAM files contain tag @RG line with the following code (no if nothing appears):\nsamtools view -H &lt;your BAM file&gt; | grep '@RG'\nCheck whether your BAM files are sorted:\nsamtools view -H &lt;your BAM file&gt; | grep '@HD'\nSee here for how read group is defined by GATK and here for how to extract some info from FASTQ files.\nRead group defined by GATK:\n@RG ID:H0164.2  PL:illumina PU:H0164ALXX140820.2    LB:Solexa-272222    PI:0    DT:2014-08-20T00:00:00-0400 SM:NA12878  CN:BI\n\nID:&lt;flow cell name&gt;.&lt;lane number&gt;: read group identifier.\nPU:&lt;flow cell barcode&gt;.&lt;lane number&gt;.&lt;sample barcode&gt;: platform unit (optional). Flow cell barcode refers to the unique identifier for a particular flow cell. Sample barcode is a sample/library-specific identifier.\nSM:&lt;sample name&gt;: sample.\nPL:&lt;platform name&gt;: platform/technology used to produce the read, such as ILLUMINA, SOLID, LS454, HELICOS and PACBIO.\nLB:&lt;library name&gt;: DNA preparation library identifier. This can be used to identify duplicates derived from library preparation.\n\n\n# Construct tag @RG line by extracting some info from paired-end FASTQ files and their file names\n# Only compatible with Casava 1.8 format\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nadd_rg_bam_dir = \"add_rg_bam\"\nplatform = \"ILLUMINA\"\nsamtools_n_threads = 40\n\nmkpath(add_rg_bam_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nfastq_header_names = [\"instrument_name\", \"run_id\", \"flowcell_id\", \"flowcell_lane\", \"lane_tile_number\", \"tile_cluster_x_coord\",\n    \"tile_cluster_y_coord\", \"pair_member_number\", \"is_passed\", \"control_bits_on\", \"index_sequence\"]\nfor raw_fastq_file in raw_fastq_files\n    input_bam_file = joinpath(high_qual_bam_dir, replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \".chr_srt.bam\"))\n    output_bam_file = joinpath(add_rg_bam_dir, basename(input_bam_file))\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", raw_fastq_file, \" | grep -P '^@' | head -n 1\")]))\n    fastq_header_line = split(strip(strip(read(cmd, String)), '@'), r\" +\")\n    if length(fastq_header_line) == 2\n        fastq_header_values = YRUtils.BaseUtils.flatten_array(split.(fastq_header_line, \":\"))\n        if length(fastq_header_values) == length(fastq_header_names)\n            fastq_header_dict = Dict(zip(fastq_header_names, fastq_header_values))\n            rg_line = string(\"@RG\\\\tID:\", fastq_header_dict[\"flowcell_id\"], \".\", fastq_header_dict[\"flowcell_lane\"],\n                \"\\\\tPL:\", platform, \"\\\\tSM:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \"\"),\n                \"\\\\tLB:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \"\"))\n            if isfile(input_bam_file) && !isfile(output_bam_file)\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"samtools addreplacerg -@ \", samtools_n_threads, \" -r '\", rg_line, \"' -o \",\n                        output_bam_file, \" \", input_bam_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                @info string(\"input BAM file (\", input_bam_file, \") is invalid or output BAM file (\", output_bam_file, \") has already existed, you can add @RG line yourself: \", rg_line)\n            end\n        else\n            @error \"unsupported FASTQ header format\"\n        end\n    else\n        @error \"unsupported FASTQ header format\"\n    end\nend\n\n\n\n9.2 Remove duplicate reads\n\nusing YRUtils\n\nadd_rg_bam_dir = \"add_rg_bam\"\nmark_dup_bam_dir = \"mark_dup_bam\"\nrm_dup_bam_dir = \"rm_dup_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\npicard_path = \"/data/softwares/picard_v3.3.0/picard.jar\"\nsamtools_n_threads = 40\n\nmkpath(mark_dup_bam_dir)\nmkpath(rm_dup_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(add_rg_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    mark_dup_bam_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".mark_dup.bam\"))\n    mark_dup_metrics_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".metrics.txt\"))\n    rm_dup_bam_file = joinpath(rm_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".rm_dup.bam\"))\n\n    cmd = pipeline(\n        Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"java -jar \", picard_path, \" MarkDuplicates --INPUT \", bam_file, \" --OUTPUT \", mark_dup_bam_file,\n                    \" --METRICS_FILE \", mark_dup_metrics_file, \" --VALIDATION_STRINGENCY LENIENT \",\n                    \" --USE_JDK_DEFLATER true --USE_JDK_INFLATER true --ASSUME_SORT_ORDER coordinate \",\n                    \" --REMOVE_DUPLICATES false --TMP_DIR \", tmp_dir)]),\n        );\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\n\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" -F 1804 -f 2 -b \",\n                mark_dup_bam_file, \" -o \", rm_dup_bam_file)]));\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-fragment-length-distributions",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-fragment-length-distributions",
    "title": "Cut&Tag analysis pipeline",
    "section": "10 Assess fragment length distributions",
    "text": "10 Assess fragment length distributions\n\n10.1 Extract fragment lengths\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nfrag_len_dir = \"frag_len\"\nsamtools_n_threads = 40\n\nmkpath(frag_len_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    frag_len_stat_file = joinpath(frag_len_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".frag_len_stat.tsv\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" \", bam_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' 'function abs(x) {return ((x &lt; 0.0) ? -x : x)} {print abs($9)}' \",\n                raw\" | sort -n | uniq -c | awk -v OFS='\\t' '{print $2,$1/2}' &gt; \", frag_len_stat_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n\n\n\n10.2 Visualize fragment length distibutions\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(vroom)\n\nfrag_len_dir &lt;- \"frag_len\"\n\nfrag_len_stat_files &lt;- list.files(frag_len_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\ndf &lt;- tibble()\nfor (frag_len_stat_file in frag_len_stat_files) {\n    tmp_df &lt;- vroom(frag_len_stat_file, col_names = c(\"frag_len\", \"count\")) %&gt;%\n        mutate(sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(frag_len_stat_file)))\n    df &lt;- bind_rows(df, tmp_df)\n}\ndf &lt;- df %&gt;% arrange(sample, frag_len, count)\n\np &lt;- ggplot(df, aes(frag_len, count, color = sample)) +\n    geom_freqpoly(stat = \"identity\", linewidth = 0.5) +\n    labs(\n        x = \"Fragment length\",\n        y = \"Count\", color = \"Sample\"\n    ) +\n    theme_classic(base_family = \"Arial\", base_size = 20)\nppreview(p, file = file.path(frag_len_dir, \"frag_len_dist.freqpoly.pdf\"))"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-the-reproducibility-of-replicates",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-the-reproducibility-of-replicates",
    "title": "Cut&Tag analysis pipeline",
    "section": "11 Assess the reproducibility of replicates",
    "text": "11 Assess the reproducibility of replicates\n\n11.1 Convert BAM into BED\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bed_dir = \"bam2bed\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam2bed_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    bed_file = joinpath(bam2bed_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".bed\"))\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" -o \", tmp_name_srt_bam_file, \" \", bam_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"bedtools bamtobed -bedpe -i \", tmp_name_srt_bam_file,\n            raw\" | awk -v FS='\\t' -v OFS='\\t' '$1 == $4 && $6 - $2 &lt; 1000 {print $0}' \",\n            raw\" | cut -f 1,2,6 | sort -k1,1 -k2,2n -k3,3n &gt; \", bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm(tmp_name_srt_bam_file)\nend\n\n\n\n11.2 Aggregate fragments into bin counts\n\n# We use the middle point of each fragment to infer which bin this fragment belongs to.\n# Calculate stategy:\n# w - window size (e.g. 500)\n# $1 - seqname\n# $2 - start\n# $3 - end\n# int() - round down\n# int(($2 + $3)/(2*w))*w + w/2\n# e.g. all middle points belonging to the left-closed interval [1000, 1500) will have the same quotient 2 by dividing 500,\n# and then, by multiplying 500, and then, by adding 500/2, and finally, the quantity is 1250.\n# Finally we use the vaule 1250 to represent all middle points belonging to [1000, 1500),\n# and then we just need to count the number of 1250 to know how many fragments are enriched in the interval [1000, 1500)\n\nusing YRUtils\n\nbam2bed_dir = \"bam2bed\"\nbin_width = 500\n\nbed_files = YRUtils.BaseUtils.list_files(bam2bed_dir, r\"\\.bed$\", recursive=false, full_name=true)\nfor bed_file in bed_files\n    frag_bin_count_file = joinpath(bam2bed_dir, replace(basename(bed_file), r\"\\.bed$\" =&gt; string(\".bin\", bin_width, \".tsv\")))\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", bed_file, \" | awk -v w=\", bin_width,\n                raw\" -v FS='\\t' -v OFS='\\t' '{print $1, int(($2 + $3)/(2*w))*w + w/2}' \",\n                raw\" | sort -k1,1V -k2,2n | uniq -c | awk -v OFS='\\t' '{print $2,$3,$1}' \",\n                \" | sort -k1,1V -k2,2n &gt; \", frag_bin_count_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n\n\n\n11.3 Assess the reproducibility of replicates\n# At this step, we convert raw counts to Counts Per Million (CPM) to normalize the sequencing depth\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(psych)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggprism)\nlibrary(ggalign)\nlibrary(YRUtils)\n\nbam2bed_dir &lt;- \"bam2bed\"\nqc_dir &lt;- \"qc\"\nbin_width &lt;- 500\ncpm_th &lt;- 1\n\ndir.create(qc_dir, showWarnings = FALSE, recursive = FALSE)\nfiles &lt;- list.files(bam2bed_dir, pattern = paste0(\"\\\\.bin\", bin_width, \"\\\\.tsv$\"), full.names = TRUE, recursive = FALSE)\ndf &lt;- tibble()\nfor (file in files) {\n    tmp_df &lt;- vroom(file, col_names = c(\"seqname\", \"mid_point\", \"count\")) %&gt;%\n        mutate(\n            id = paste0(seqname, \":\", mid_point),\n            sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(file))\n        ) %&gt;%\n        select(id, count, sample)\n    df &lt;- bind_rows(df, tmp_df)\n}\ndf &lt;- df %&gt;%\n    group_by(sample) %&gt;%\n    mutate(cpm = count / sum(count) * 1e6) %&gt;%\n    pivot_wider(id_cols = \"id\", names_from = \"sample\", values_from = \"cpm\", values_fill = 0) %&gt;%\n    select(-all_of(\"id\"))\ndf &lt;- log2(df[rowSums(df &gt;= cpm_th) &gt;= 1, ] + 1)\n\n# Correlation\ncor_res &lt;- corr.test(df, use = \"pairwise\", method = \"pearson\", adjust = \"BH\")\n\np &lt;- ggheatmap(\n    cor_res$r,\n    width = ncol(cor_res$r) * unit(10, \"mm\"),\n    height = nrow(cor_res$r) * unit(10, \"mm\")\n) +\n    scheme_align(free_spaces = \"t\") +\n    scale_fill_gradient2(\n        low = \"blue\", mid = \"white\", high = \"red\",\n        midpoint = 0, limits = c(-1, 1),\n        breaks = c(-1, -0.5, 0, 0.5, 1)\n    ) +\n    labs(fill = \"R\") +\n    guides(x = guide_axis(angle = 45)) +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    anno_top() +\n    ggalign(\n        data = gsub(\"_rep\\\\d+$\", \"\", colnames(cor_res$r)),\n        size = unit(4, \"mm\")\n    ) +\n    geom_tile(aes(y = 1, fill = factor(value))) +\n    scale_y_continuous(breaks = NULL, name = NULL, expand = expansion(0)) +\n    labs(fill = \"Sample\") +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    with_quad(scheme_align(guides = \"t\"), NULL)\nppreview(p, file = file.path(qc_dir, paste0(\"correlation.bin\", bin_width, \".pdf\")))\n\n# PCA\npca &lt;- PCA(t(df), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord &lt;- as.data.frame(pca$ind$coord)\npca_coord$sample &lt;- row.names(pca_coord)\npca_coord$group &lt;- factor(gsub(\"_rep\\\\d+$\", \"\", pca_coord$sample))\npca_eig &lt;- as.data.frame(pca$eig)\n\np &lt;- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 4) +\n    xlab(paste0(\"PC1 (\", round(pca_eig[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(pca_eig[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25) +\n    theme_prism(base_family = \"Arial\", border = TRUE, base_size = 20)\nppreview(p, file = file.path(qc_dir, paste0(\"pca.bin\", bin_width, \".pdf\")))"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-signal-enrichment-profile",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-signal-enrichment-profile",
    "title": "Cut&Tag analysis pipeline",
    "section": "12 Plot signal enrichment profile",
    "text": "12 Plot signal enrichment profile\nIf controls are far more enriched than treatments, you may need to call peaks of treatments without using controls.\nYou can try to call peaks of controls and to explore the reason why these regions are enriched (maybe you should remove these regions from peaks of treatments if needed).\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nqc_dir = \"qc\"\nsambamba_n_threads = 40\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 500\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"plotFingerprint -b \", join(bam_files, \" \"),\n            \" --labels \", join(replace.(basename.(bam_files), r\"\\.\\w+\\.bam$\" =&gt; \"\"), \" \"),\n            \" --outQualityMetrics \", joinpath(qc_dir, \"plotfingerprint_quality_metrics.log\"),\n            \" --skipZeros --minMappingQuality \", map_qual,\n            \" --numberOfProcessors \", deeptools_n_threads,\n            \" --binSize \", bin_size,\n            \" --plotFile \", joinpath(qc_dir, \"plotfingerprint.pdf\"),\n            \" --outRawCounts \", joinpath(qc_dir, \"plotfingerprint.tsv\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#convert-bam-into-bigwig",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#convert-bam-into-bigwig",
    "title": "Cut&Tag analysis pipeline",
    "section": "13 Convert BAM into BigWig",
    "text": "13 Convert BAM into BigWig\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bw_dir = \"bam2bw\"\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 10\nnorm_method = \"RPKM\"\neffective_genome_size = 3372855573\n\nmkpath(bam2bw_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    bw_file = joinpath(bam2bw_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".bw\"))\n    if !isfile(bw_file)\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"bamCoverage -b \", bam_file, \" -o \", bw_file,\n                    \" --numberOfProcessors \", deeptools_n_threads,\n                    \" --binSize \", bin_size,\n                    \" --normalizeUsing \", norm_method,\n                    \" --effectiveGenomeSize \", effective_genome_size,\n                    \" --minMappingQuality \", map_qual,\n                    \" --extendReads\")]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-tss-enrichment-profile",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-tss-enrichment-profile",
    "title": "Cut&Tag analysis pipeline",
    "section": "14 Plot TSS enrichment profile",
    "text": "14 Plot TSS enrichment profile\n\n14.1 Extract transcription start sites\nlibrary(tidyverse)\nlibrary(rtracklayer)\nlibrary(vroom)\n\ngff_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\ngff_format &lt;- \"gtf\"\nseq_type &lt;- \"transcript\"\nid_col &lt;- \"transcript_id\"\n\ngff &lt;- import(gff_file, format = gff_format) %&gt;%\n    as.data.frame() %&gt;%\n    as_tibble()\ndf &lt;- gff %&gt;%\n    filter(type == seq_type) %&gt;%\n    mutate(\n        tss_start = if_else(strand == \"-\", end, start - 1),\n        tss_end = if_else(strand == \"-\", end + 1, start),\n        score = 0\n    ) %&gt;%\n    select(all_of(c(\"seqnames\", \"tss_start\", \"tss_end\", id_col, \"score\", \"strand\"))) %&gt;%\n    distinct()\nvroom_write(df,\n    file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".transcription_start_sites.bed\", gff_file),\n    col_names = FALSE, append = FALSE\n)\n\n\n14.2 Plot TSS enrichment profile\n\nusing YRUtils\n\nbam2bw_dir = \"bam2bw\"\ntss_file = \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.transcription_start_sites.bed\"\nqc_dir = \"qc\"\ndeeptools_n_threads = 40\nbin_size = 10\nbefore_length = 2000\nafter_length = 2000\n\nbw_files = YRUtils.BaseUtils.list_files(bam2bw_dir, r\"\\.bw$\", recursive=false, full_name=true)\nfor bw_file in bw_files\n    tss_cov_mat_file = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".mat.txt\"))\n    tss_cov_mat_gz = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".mat.txt.gz\"))\n    tss_cov_heatmap_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".heatmap.pdf\"))\n    tss_cov_profile_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".profile.pdf\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"computeMatrix reference-point --referencePoint TSS --numberOfProcessors \", deeptools_n_threads,\n                \" -S \", bw_file, \" -R \", tss_file, \" --binSize \", bin_size,\n                \" --beforeRegionStartLength \", before_length, \" --afterRegionStartLength \", after_length,\n                \" --skipZeros --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"),\n                \" --outFileNameMatrix \", tss_cov_mat_file, \" --outFileName \", tss_cov_mat_gz)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotHeatmap -m \", tss_cov_mat_gz, \" -o \", tss_cov_heatmap_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotProfile -m \", tss_cov_mat_gz, \" -o \", tss_cov_profile_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#call-peaks-with-macs",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#call-peaks-with-macs",
    "title": "Cut&Tag analysis pipeline",
    "section": "15 Call peaks with MACS",
    "text": "15 Call peaks with MACS\n\n15.1 Pairwise merging of biological replicates\ne.g. if you have three biological replicates for a sample, you will get three pooled BAM files: rep1_vs_rep2, rep1_vs_rep3, and rep2_vs_rep3.\n\nusing YRUtils\nusing Combinatorics\nusing NaturalSort\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nsamtools_n_threads = 40\nsambamba_n_threads = 40\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\ndict = Dict{String,Vector{String}}()\nfor bam_file in bam_files\n    sample = replace(basename(bam_file), r\"_rep\\d+\\.\\w+\\.bam$\" =&gt; \"\")\n    if !haskey(dict, sample)\n        dict[sample] = [bam_file]\n    else\n        push!(dict[sample], bam_file)\n    end\nend\nfor sample in keys(dict)\n    combns = collect(combinations(dict[sample], 2))\n    for combn in combns\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"samtools merge -t \", samtools_n_threads,\n                    \" -o \", joinpath(rm_dup_bam_dir,\n                        string(join(sort(replace.(basename.(combn), r\"\\.\\w+\\.bam$\" =&gt; \"\"), lt=natural), \"_vs_\"),\n                            \".rm_dup.bam\")),\n                    \" \", join(combn, \" \"))]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\n\n\n\n15.2 Call peaks with MACS\nIf you want to perform IDR analysis, then use a relax threshold (e.g. p-value = 0.01) is suitable.\nIf you only want to perform naive overlapping, then use a stringent threshold (e.g. q-value = 0.05) is suitable.\n\n15.2.1 Call peaks with controls (using p-value)\n\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" =&gt; \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.2 Call peaks with controls (using q-value)\n\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" =&gt; \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.3 Call peaks without controls (using p-value)\n\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" =&gt; \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.4 Call peaks without controls (using q-value)\n\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" =&gt; \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#naive-overlapping",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#naive-overlapping",
    "title": "Cut&Tag analysis pipeline",
    "section": "16 Naive overlapping",
    "text": "16 Naive overlapping\n\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_peaks\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n]\ninput_dir = \"peak_without_ctl_qval0.05\"\noutput_dir = \"naive_overlap_without_ctl_qval0.05\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools intersect -wo \",\n                \" -a \", joinpath(input_dir, narrow_peak[1]),\n                \" -b \", joinpath(input_dir, narrow_peak[2]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 &gt;= 0.5) || ($21/s2 &gt;= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq \",\n                \" | bedtools intersect -wo \",\n                \" -a stdin -b \", joinpath(input_dir, narrow_peak[3]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 &gt;= 0.5) || ($21/s2 &gt;= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq | pigz -nc &gt; \", joinpath(output_dir, narrow_peak[4]))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#idr-analysis",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#idr-analysis",
    "title": "Cut&Tag analysis pipeline",
    "section": "17 IDR analysis",
    "text": "17 IDR analysis\n\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_prefix\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01\",\n    ],\n]\ninput_dir = \"peak_without_ctl_pval0.01\"\noutput_dir = \"idr_without_ctl_pval0.01\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nidr_threshold = 0.05\npeak_type = \"narrowPeak\"\nrank = \"p.value\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    peak1_file = joinpath(input_dir, narrow_peak[2])\n    peak2_file = joinpath(input_dir, narrow_peak[3])\n    pooled_peak_file = joinpath(input_dir, narrow_peak[1])\n\n    prefix = joinpath(output_dir, string(narrow_peak[4], \".idr\", idr_threshold))\n    idr_peak_file = string(prefix, \".\", peak_type, \".gz\")\n    idr_log_file = string(prefix, \".log\")\n    idr_12col_bed_file = string(prefix, \".\", peak_type, \".12-col.bed.gz\")\n    idr_out_file = string(prefix, \".unthresholded-peaks.txt\")\n    idr_tmp_file = string(prefix, \".unthresholded-peaks.txt.tmp\")\n    idr_out_gz_file = string(prefix, \".unthresholded-peaks.txt.gz\")\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"idr \",\n                \" --samples \", peak1_file, \" \", peak2_file,\n                \" --peak-list \", pooled_peak_file,\n                \" --input-file-type \", peak_type,\n                \" --output-file \", idr_out_file,\n                \"  --rank \", rank,\n                \" --soft-idr-threshold \", idr_threshold,\n                \" --log-output-file \", idr_log_file,\n                \" --plot --use-best-multisummit-IDR\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", idr_out_file, \" \", chrsz, \" \", idr_tmp_file, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    if rank == \"signal.value\"\n        col = 7\n    elseif rank == \"p.value\"\n        col = 8\n    elseif rank == \"q.value\"\n        return 9\n    else\n        @error \"invalid score ranking method\"\n    end\n    minus_log10_threshold = -log10(idr_threshold)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '$12&gt;=\", minus_log10_threshold,\n                raw\" {if ($2&lt;0) $2=0; print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' \",\n                \" | sort -k1,1 -k2,2n | uniq | sort -grk\", col, \",\", col,\n                \" | pigz -nc &gt; \", idr_12col_bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"zcat \", idr_12col_bed_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10}' \",\n                \" | pigz -nc &gt; \", idr_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                \" | pigz -nc &gt; \", idr_out_gz_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([idr_out_file, idr_tmp_file, idr_12col_bed_file, string(idr_out_file, \".noalternatesummitpeaks.png\")])\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#pairwise-overlapping-statistics",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#pairwise-overlapping-statistics",
    "title": "Cut&Tag analysis pipeline",
    "section": "18 Pairwise overlapping statistics",
    "text": "18 Pairwise overlapping statistics\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggprism)\nlibrary(YRUtils)\nlibrary(patchwork)\n\n# [\"peaks\", \"peaks\", ...]\nnarrow_peaks &lt;- c(\n    \"Satb2_rep1_vs_Satb2_rep2.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep1_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep2_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\"\n)\ninput_dir &lt;- \"naive_overlap_without_ctl_pval0.01\"\noverlap_stat_dir &lt;- \"overlap_stat\"\n\ndir.create(overlap_stat_dir)\ncombns &lt;- combn(narrow_peaks, 2)\nfor (i in seq_len(ncol(combns))) {\n    narrow_peak_pair &lt;- file.path(input_dir, combns[, i])\n    raw_df1 &lt;- vroom(gzfile(narrow_peak_pair[1]), col_names = FALSE) %&gt;%\n        select(X1, X2, X3) %&gt;%\n        arrange(X1, X2, X3) %&gt;%\n        distinct()\n    raw_df2 &lt;- vroom(gzfile(narrow_peak_pair[2]), col_names = FALSE) %&gt;%\n        select(X1, X2, X3) %&gt;%\n        arrange(X1, X2, X3) %&gt;%\n        distinct()\n    overlapped_df &lt;- bt.intersect(\n        a = raw_df1,\n        b = raw_df2,\n        wo = TRUE\n    )\n    overlapped_df1 &lt;- overlapped_df %&gt;%\n        select(V1, V2, V3, V7) %&gt;%\n        arrange(V1, V2, V3) %&gt;%\n        distinct() %&gt;%\n        mutate(\n            percent = V7 / (V3 - V2),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n    overlapped_df2 &lt;- overlapped_df %&gt;%\n        select(V4, V5, V6, V7) %&gt;%\n        arrange(V4, V5, V6) %&gt;%\n        distinct() %&gt;%\n        mutate(\n            percent = V7 / (V6 - V5),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n\n    qc_metrics &lt;- paste0(\n        \"&gt;&gt;&gt; \", paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \" vs. \"), \": \\n\",\n        \"&gt;&gt; \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[1])), \": \\n\",\n        \"&gt; The number of peaks in total: \", nrow(raw_df1), \"\\n\",\n        \"&gt; The number of peaks overlapped: \", nrow(overlapped_df1), \"\\n\",\n        \"&gt; Percentage: \", round(nrow(overlapped_df1) / nrow(raw_df1), digits = 4), \"\\n\",\n        \"&gt;&gt; \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[2])), \": \\n\",\n        \"&gt; The number of peaks in total: \", nrow(raw_df2), \"\\n\",\n        \"&gt; The number of peaks overlapped: \", nrow(overlapped_df2), \"\\n\",\n        \"&gt; Percentage: \", round(nrow(overlapped_df2) / nrow(raw_df2), digits = 4)\n    )\n\n    vroom_write_lines(qc_metrics,\n        file = file.path(\n            overlap_stat_dir,\n            paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".txt\")\n        ),\n        append = FALSE\n    )\n\n    p1 &lt;- ggplot(\n        overlapped_df1 %&gt;%\n            group_by(interval) %&gt;%\n            reframe(n = n()) %&gt;%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"1 vs. 2\") +\n        theme_prism()\n\n    p2 &lt;- ggplot(\n        overlapped_df2 %&gt;%\n            group_by(interval) %&gt;%\n            reframe(n = n()) %&gt;%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"2 vs. 1\") +\n        theme_prism()\n\n    ppreview(p1 | p2, file = file.path(\n        overlap_stat_dir,\n        paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".pdf\")\n    ))\n}"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-signal-tracks-with-macs",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-signal-tracks-with-macs",
    "title": "Cut&Tag analysis pipeline",
    "section": "19 Generate signal tracks with MACS",
    "text": "19 Generate signal tracks with MACS\n\nusing YRUtils\n\nbdg_dir = \"peak_without_ctl_qval0.05\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n\nbdg_files = YRUtils.BaseUtils.list_files(bdg_dir, r\"\\.bdg$\", recursive=false, full_name=true)\nsamples = unique(replace.(basename.(bdg_files), r\"(_control_lambda|_treat_pileup).bdg$\" =&gt; \"\"))\nfor sample in samples\n    bdg_prefix = joinpath(bdg_dir, sample)\n    treat_bdg_file = string(bdg_prefix, \"_treat_pileup.bdg\")\n    ctl_bdg_file = string(bdg_prefix, \"_control_lambda.bdg\")\n    fc_bdg_file = string(bdg_prefix, \".fc.signal.bdg\")\n    fc_srt_bdg_file = string(bdg_prefix, \".fc.signal.srt.bdg\")\n    fc_bw_file = string(bdg_prefix, \".fc.signal.bw\")\n\n    # For fold enrichment signal tracks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 bdgcmp \",\n                \" -t \", treat_bdg_file,\n                \" -c \", ctl_bdg_file,\n                \" --o-prefix \", bdg_prefix,\n                \" -m FE\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools slop -i \", string(bdg_prefix, \"_FE.bdg\"), \" -g \", chrsz, \" -b 0 \",\n                \" | bedClip stdin \", chrsz, \" \", fc_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort and remove any overlapping regions in bedgraph by comparing two lines in a row\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k1,1 -k2,2n \", fc_bdg_file,\n                raw\" | awk -v OFS='\\t' '{if (NR==1 || NR&gt;1 && (prev_chr!=$1 || prev_chr==$1 && prev_chr_e&lt;=$2)) {print $0}; prev_chr=$1; prev_chr_e=$3;}' &gt; \", fc_srt_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedGraphToBigWig \", fc_srt_bdg_file, \" \", chrsz, \" \", fc_bw_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([fc_bdg_file, fc_srt_bdg_file, treat_bdg_file, ctl_bdg_file, string(bdg_prefix, \"_FE.bdg\")])\nend"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#extending-peaks-on-both-sides-from-summits",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#extending-peaks-on-both-sides-from-summits",
    "title": "Cut&Tag analysis pipeline",
    "section": "20 Extending peaks on both sides from summits",
    "text": "20 Extending peaks on both sides from summits\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nextend_width &lt;- 250\nmerge_dist &lt;- 10\nchrsz_file &lt;- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\npeak_files &lt;- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\"\n)\n\nchrsz &lt;- vroom(chrsz_file, col_names = FALSE)\nfor (peak_file in peak_files) {\n    tmp_df &lt;- vroom(gzfile(peak_file), col_names = FALSE) %&gt;%\n        mutate(\n            summit_seqname = X1,\n            summit_start = X2 + X10,\n            summit_end = summit_start,\n        ) %&gt;%\n        select(summit_seqname, summit_start, summit_end) %&gt;%\n        arrange(summit_seqname, summit_start, summit_end) %&gt;%\n        distinct()\n    df &lt;- bt.slop(i = tmp_df, g = chrsz, b = extend_width) %&gt;%\n        distinct() %&gt;%\n        arrange(V1, V2, V3)\n    df &lt;- bt.merge(df, d = merge_dist) %&gt;%\n        mutate(\n            V4 = paste0(V1, \":\", V2, \"-\", V3),\n            V5 = 1000,\n            V6 = \"+\"\n        )\n    vroom_write(df,\n        file = gsub(\"\\\\.narrowPeak\\\\.gz$\", paste0(\".summits.b\", 2 * extend_width, \".bed\"), peak_file),\n        col_names = FALSE,\n        append = FALSE\n    )\n}"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-final-peak-sets",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-final-peak-sets",
    "title": "Cut&Tag analysis pipeline",
    "section": "21 Generate final peak sets",
    "text": "21 Generate final peak sets\nScheme 1: (recommended)\n\nCall peaks with MACS using p-value = 0.05\nPerform pairwise IDR analysis using idr threshold = 0.05\nMerge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\nScheme 2:\n\nCall peaks with MACS using q-value = 0.05\nPerform pairwise naive overlapping\nMerge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nmerge_dist &lt;- 10\noutput_dir &lt;- \"final_peak\"\npeak_files &lt;- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\"\n)\noutput_file &lt;- \"Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\n\ndir.create(output_dir)\ndf &lt;- tibble()\nfor (peak_file in peak_files) {\n    df &lt;- bind_rows(\n        df,\n        vroom(gzfile(peak_file), col_names = FALSE) %&gt;%\n            select(X1, X2, X3)\n    )\n}\ndf &lt;- distinct(df) %&gt;%\n    arrange(X1, X2, X3)\ndf &lt;- bt.merge(df, d = merge_dist) %&gt;%\n    mutate(\n        V4 = paste0(V1, \":\", V2, \"-\", V3),\n        V5 = 1000,\n        V6 = \"+\"\n    )\nvroom_write(df, file = file.path(output_dir, output_file), col_names = FALSE, append = FALSE)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#de-novo-motif-finding-with-homer",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#de-novo-motif-finding-with-homer",
    "title": "Cut&Tag analysis pipeline",
    "section": "22 De novo motif finding with Homer",
    "text": "22 De novo motif finding with Homer\n\n22.1 De novo motif finding with Homer\n\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/homer/de_novo/narrow_peak\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" =&gt; \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' &gt; \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" \", extra_args, \" \", if !isempty(known_motifs_file)\n                string(\" -mknown \", known_motifs_file)\n            else\n                \"\"\n            end)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\n\n\n22.2 Finding instances of specific motifs\n\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\noutput_file = \"motif_instances.txt\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" =&gt; \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' &gt; \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" -find \", known_motifs_file, \" \", extra_args, \" &gt; \", joinpath(output_dir, output_file))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#peak-annotation",
    "href": "posts/Cut&Tag/cuttag_analysis_pipeline/index.html#peak-annotation",
    "title": "Cut&Tag analysis pipeline",
    "section": "23 Peak annotation",
    "text": "23 Peak annotation\n\n23.1 Make TxDb object from GFF3/GTF file\nlibrary(txdbmaker)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(GenomeInfoDb)\n\nanno_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\nchrsz_file &lt;- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nanno_format &lt;- \"gtf\"\norganism &lt;- \"Mus musculus\"\n# Sugar glider (Petaurus breviceps): 34899\n# Mouse (Mus musculus): 10090\ntaxonomy_id &lt;- 10090\ngenome &lt;- \"mm10\"\ncircular_chrs &lt;- c(\"chrM\")\n\nchrsz &lt;- vroom(chrsz_file, col_names = FALSE)\nseqinfo &lt;- Seqinfo(\n    seqnames = chrsz$X1,\n    seqlengths = chrsz$X2,\n    isCircular = if_else(chrsz$X1 %in% circular_chrs, TRUE, FALSE),\n    genome = genome\n)\ntxdb &lt;- makeTxDbFromGFF(\n    file = anno_file,\n    format = anno_format,\n    organism = organism,\n    taxonomyId = taxonomy_id,\n    chrominfo = seqinfo\n)\n# loadDB()\nsaveDb(txdb, file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".TxDb.sqlite\", anno_file))\n\n\n23.2 Profile of peaks binding to TSS/body/TTS regions\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggprism)\nlibrary(ggplot2)\n\npeak_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir &lt;- \"final_peak/peak_anno/narrow_peak\"\nupstream_dist &lt;- 2000\ndownstream_dist &lt;- 2000\nby_what &lt;- \"gene\"\ntypes &lt;- c(\"start_site\", \"body\", \"end_site\")\nnbin &lt;- 400\n\npeak &lt;- readPeakFile(peak_file)\ntxdb &lt;- loadDb(txdb_file)\nfor (type in types) {\n    profile_p &lt;- plotPeakProf2(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme_prism(\n        base_size = 14,\n        base_family = \"Arial\",\n        border = TRUE\n    )\n\n    profile_p_file &lt;- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".profile.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(profile_p, file = profile_p_file)\n\n    heatmap_p &lt;- peakHeatmap(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme(\n        text = element_text(family = \"Arial\", size = 14)\n    )\n\n    heatmap_p_file &lt;- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".heatmap.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(heatmap_p, file = heatmap_p_file)\n}\n\n\n23.3 Peak annotation\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggplot2)\nlibrary(ggprism)\nlibrary(vroom)\n\npeak_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir &lt;- \"final_peak/peak_anno/narrow_peak\"\ntss_region &lt;- c(-2000, 2000)\nby_what &lt;- \"transcript\"\n\npeak &lt;- readPeakFile(peak_file)\ntxdb &lt;- loadDb(txdb_file)\npeak_anno &lt;- annotatePeak(\n    peak = peak,\n    tssRegion = tss_region,\n    TxDb = txdb,\n    level = by_what\n)\n\npeak_anno_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno.tsv\"),\n        basename(peak_file)\n    )\n)\nvroom_write(as.data.frame(peak_anno),\n    file = peak_anno_file,\n    col_names = TRUE, append = FALSE\n)\n\npeak_anno_stat_df &lt;- getAnnoStat(peak_anno)\n\npie_p &lt;- ggplot(peak_anno_stat_df, aes(x = \"\", y = Frequency, fill = Feature)) +\n    geom_col() +\n    coord_polar(theta = \"y\") +\n    theme_void(base_size = 20, base_family = \"Arial\")\n\npie_p_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_pie.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(pie_p, file = pie_p_file)\n\ntss_distribution_p &lt;- plotDistToTSS(peak_anno) +\n    labs(title = NULL) +\n    theme_prism(border = TRUE, base_size = 20, base_family = \"Arial\") +\n    theme(legend.title = element_text())\n\ntss_distribution_p_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_tss_distribution.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(tss_distribution_p, file = tss_distribution_p_file)\n\n\n23.4 Functional enrichment analysis\nlibrary(vroom)\nlibrary(magrittr)\nlibrary(clusterProfiler)\nlibrary(AnnotationDbi)\nlibrary(tidyverse)\n\npeak_anno_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/peak_anno/narrow_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.transcript.anno.tsv\"\nmpt_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.trna.ercc.phix.gtf.gz.gene_id_name_mapping_table.tsv\"\norgdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/org.Mm.eg.db.sqlite\"\n\nmpt_df &lt;- vroom(mpt_file) %&gt;%\n    select(gene_id, gene_name) %&gt;%\n    distinct()\npeak_anno_df &lt;- vroom(peak_anno_file) %&gt;%\n    inner_join(mpt_df, by = c(\"geneId\" = \"gene_id\")) %&gt;%\n    select(seqnames, start, end, V4, V5, V6, gene_name, distanceToTSS) %&gt;%\n    set_colnames(c(\"seqname\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"gene_name\", \"dist_to_tss\")) %&gt;%\n    distinct()\nvroom_write(peak_anno_df,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.with_names.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)\n\norgdb &lt;- loadDb(orgdb_file)\nego &lt;- enrichGO(\n    unique(na.omit(peak_anno_df[[\"gene_name\"]])),\n    OrgDb = orgdb,\n    keyType = \"SYMBOL\",\n    ont = \"ALL\",\n    pvalueCutoff = 0.05,\n    qvalueCutoff = 0.05,\n    pAdjustMethod = \"BH\",\n    minGSSize = 10,\n    maxGSSize = 1000,\n    readable = FALSE,\n    pool = FALSE\n)\nego\nnrow(ego@result)\nsaveRDS(ego, file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.rds\", peak_anno_file))\nvroom_write(ego@result,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "",
    "text": "wget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.fna.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gff.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gtf.gz"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#download-ferret-mustela-putorius-furo-9669-reference-files",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#download-ferret-mustela-putorius-furo-9669-reference-files",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "",
    "text": "wget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.fna.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gff.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gtf.gz"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#extract-transcript-sequences",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#extract-transcript-sequences",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "2 Extract transcript sequences",
    "text": "2 Extract transcript sequences\n\n# -w: write a fasta file with spliced exons for each transcript\ngffread -w GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -g GCF_011764305.1_ASM1176430v1.1_genomic.fna GCF_011764305.1_ASM1176430v1.1_genomic.gff"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#build-blast-database",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#build-blast-database",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "3 Build blast database",
    "text": "3 Build blast database\n\nmakeblastdb -in GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -parse_seqids -taxid 9669 -blastdb_version 5 -title \"GCF_011764305.1_ASM1176430v1.1 Ferret (Mustela putorius furo) spliced transcripts\" -dbtype nucl"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#prepare-mrna-microarray-data",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#prepare-mrna-microarray-data",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "4 Prepare mRNA microarray data",
    "text": "4 Prepare mRNA microarray data\n\nlibrary(GEOquery)\nlibrary(limma)\nlibrary(tidyverse)\nlibrary(vroom)\n\ngse_accession &lt;- \"GSE60687\"\nout_dir &lt;- \"./data/microarray\"\n\ngse &lt;- getGEO(gse_accession, GSEMatrix = T, getGPL = T)\nsample_df &lt;- gse[[paste0(gse_accession, \"_series_matrix.txt.gz\")]]@phenoData@data %&gt;%\n    mutate(\n        sample_id = paste0(\n            str_extract_all(description, \"A(17|19)\", simplify = T), \".\",\n            str_extract_all(description, \"[A-Z]{2,}\", simplify = T), \".\",\n            str_extract_all(description, \"^\\\\d\", simplify = T),\n            str_extract_all(description, \"_2\", simplify = T)\n        ),\n        sample_file = file.path(out_dir, paste0(sample_id, \".txt.gz\")),\n        wget_cmd = paste0(\"wget -c -O \", sample_file, \" \", supplementary_file)\n    ) %&gt;%\n    arrange(sample_id)\n\nsapply(sample_df$wget_cmd, function(x) {\n    system(x)\n})\n\nvroom_write(sample_df, file = file.path(out_dir, \"samples.tsv\"))\n\ngpl &lt;- getGEO(gse[[paste0(gse_accession, \"_series_matrix.txt.gz\")]]@annotation)\ngpl_fna &lt;- filter(gpl@dataTable@table, SPOT_ID != \"CONTROL\") %&gt;%\n    mutate(fna = paste0(\"&gt;\", ID, \":\", COL, \":\", ROW, \":\", NAME, \":\", CONTROL_TYPE, \":\", ACCESSION_STRING, \":\", CHROMOSOMAL_LOCATION, \"\\n\", SEQUENCE)) %&gt;%\n    pull(fna) %&gt;%\n    unique()\n\nvroom_write(gpl@dataTable@table, file = file.path(out_dir, \"gpl.tsv\"))\nvroom_write_lines(gpl_fna, file = file.path(out_dir, \"gpl.fna\"))"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#run-blastn",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#run-blastn",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "5 Run blastn",
    "text": "5 Run blastn\n\n# GPL probe sequences against transcripts\nblastn -task megablast -db ../genome/blastdb/GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -query gpl.fna -outfmt \"6 qseqid sseqid evalue bitscore pident qcovs stitle\" -dust no -max_target_seqs 1 -num_threads 16 -out gpl.blastn.txt"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#attach-gene-symbols-to-gpl-probes",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#attach-gene-symbols-to-gpl-probes",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "6 Attach gene symbols to GPL probes",
    "text": "6 Attach gene symbols to GPL probes\n\nlibrary(rtracklayer)\nlibrary(vroom)\nlibrary(tidyverse)\n\ngff_file &lt;- \"./data/genome/GCF_011764305.1_ASM1176430v1.1_genomic.gff\"\ngpl_blastn_file &lt;- \"./data/microarray/gpl.blastn.txt\"\nout_dir &lt;- \"./data/microarray\"\n\ngff &lt;- as.data.frame(import(gff_file, version = \"3\")) %&gt;%\n    select(all_of(c(\n        \"seqnames\", \"start\", \"end\", \"width\", \"strand\",\n        \"source\", \"type\", \"ID\", \"Dbxref\", \"Name\", \"gbkey\",\n        \"gene\", \"gene_biotype\", \"Parent\", \"transcript_id\"\n    ))) %&gt;%\n    distinct()\n\ngpl_blastn &lt;- vroom(gpl_blastn_file, col_names = c(\"qseqid\", \"sseqid\", \"evalue\", \"bitscore\", \"pident\", \"qcovs\", \"stitle\")) %&gt;%\n    distinct() %&gt;%\n    group_by(qseqid) %&gt;%\n    slice_min(evalue) %&gt;%\n    slice_max(bitscore) %&gt;%\n    slice_max(pident) %&gt;%\n    slice_max(qcovs) %&gt;%\n    ungroup()\n\ntable(duplicated(gpl_blastn$qseqid))\n\ndf &lt;- left_join(gpl_blastn, gff, by = c(\"sseqid\" = \"ID\"))\ndf &lt;- left_join(df, filter(gff, type == \"gene\") %&gt;%\n    select(all_of(c(\"seqnames\", \"gene\", \"gene_biotype\"))) %&gt;%\n    distinct(),\nby = join_by(seqnames, gene),\nsuffix = c(\".GPL_blastn\", \".Parent_gene\")\n)\n\nDbxref_ls &lt;- lapply(df$Dbxref, function(x) {\n    if (length(x) &gt; 0) {\n        y &lt;- strsplit(x, split = \":\", fixed = T) %&gt;%\n            do.call(rbind, .)\n        setNames(y[, 2], y[, 1]) %&gt;%\n            as.list() %&gt;%\n            as.data.frame()\n    } else {\n        data.frame()\n    }\n})\nDbxref_names &lt;- lapply(Dbxref_ls, names) %&gt;%\n    unlist() %&gt;%\n    unique()\nDbxref_df &lt;- lapply(Dbxref_ls, function(x) {\n    if (nrow(x) == 0) {\n        setNames(\n            rep(NA, length(Dbxref_names)),\n            Dbxref_names\n        ) %&gt;%\n            as.list() %&gt;%\n            as.data.frame()\n    } else {\n        x\n    }\n}) %&gt;% do.call(bind_rows, .)\n\nnames(Dbxref_df) &lt;- paste0(\"Dbxref_\", names(Dbxref_df))\n\ndf &lt;- bind_cols(df, Dbxref_df)\n\nvroom_write(df, file = file.path(out_dir, \"gpl.with_gene_symbols.tsv\"), delim = \"\\t\")"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#differential-analysis-using-limma",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#differential-analysis-using-limma",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "7 Differential analysis using limma",
    "text": "7 Differential analysis using limma\n\nlibrary(limma)\nlibrary(vroom)\nlibrary(tidyverse)\n\nsample_file &lt;- \"./data/microarray/samples.tsv\"\ngpl_file &lt;- \"./data/microarray/gpl.with_gene_symbols.tsv\"\nout_dir &lt;- \"./data/degs\"\n\ndir.create(out_dir)\n\ngpl &lt;- vroom(gpl_file, delim = \"\\t\")\ngpl$ProbeName &lt;- sapply(gpl$qseqid, function(x) {\n    strsplit(x, \":\")[[1]][4]\n})\ngpl &lt;- gpl %&gt;%\n    select(all_of(c(\n        \"ProbeName\", \"Dbxref_GeneID\", \"gene\",\n        \"gene_biotype.GPL_blastn\", \"gene_biotype.Parent_gene\"\n    ))) %&gt;%\n    mutate(GeneBioType = if_else(is.na(gene_biotype.Parent_gene),\n        gene_biotype.GPL_blastn,\n        gene_biotype.Parent_gene\n    )) %&gt;%\n    select(all_of(c(\"ProbeName\", \"Dbxref_GeneID\", \"gene\", \"GeneBioType\"))) %&gt;%\n    distinct()\nnames(gpl) &lt;- c(\"ProbeName\", \"EntrezID\", \"Symbol\", \"GeneBioType\")\n\ntable(duplicated(gpl$ProbeName))\n\nsample_df &lt;- vroom(sample_file)\n\n# read in data\n# here, we are reading in single-channel Agilent (foreground: median signal; background: median signal) intensity data\n# so source = \"agilent\" and green.only = T\n# here, we read in the extra column gIsWellAboveBG, which records whether the intensity of each spot is considered above the background level for that array\nx &lt;- read.maimages(\n    gsub(\n        \"~/mywd/agilent_mrna_expression_microarray_analysis_using_limma\",\n        \".\",\n        gsub(\"\\\\.gz$\", \"\", sample_df$sample_file)\n    ),\n    source = \"agilent\", green.only = T,\n    other.columns = \"gIsWellAboveBG\"\n)\nx_copy &lt;- x\n\n# gene annotation\nx$genes &lt;- left_join(x$genes, gpl, by = \"ProbeName\")\nall(x$genes$ProbeName == x_copy$genes$ProbeName)\n\n# background correction and normalization\n# at this step, we need control probes to be existed in the dataset\ny &lt;- backgroundCorrect(x, method = \"normexp\")\ny &lt;- normalizeBetweenArrays(y, method = \"quantile\")\n\n# gene filtering\n# filter out control probes\nControl &lt;- y$genes$ControlType == 1L\n# filter out probes without Symbol\nNoSymbol &lt;- is.na(y$genes$Symbol)\n# keep probes that express in at least 3 arrays (because there are at least 3 replicates in each array)\nIsExpr &lt;- rowSums(y$other$gIsWellAboveBG &gt; 0) &gt;= 3\n\nyfilt &lt;- y[!Control & !NoSymbol & IsExpr, ]\n\ngenes_colnames &lt;- c(\"EntrezID\", \"Symbol\")\nE_colnames &lt;- colnames(yfilt$E)\nexprMat &lt;- bind_cols(\n    yfilt$genes[, genes_colnames],\n    as.data.frame(yfilt$E)\n)\nexprMat_dedup &lt;- exprMat %&gt;%\n    arrange(Symbol) %&gt;%\n    group_by(EntrezID, Symbol) %&gt;%\n    reframe(across(everything(), mean))\nyfilt$genes &lt;- exprMat_dedup[, genes_colnames]\nyfilt$E &lt;- as.matrix(exprMat_dedup[, E_colnames])\n\n# differential expression\ntreatments &lt;- gsub(\"\\\\.[_0-9]+$\", \"\", sample_df$sample_id)\nlevels &lt;- unique(treatments)\ntreatments &lt;- factor(treatments, levels = levels)\ndesign &lt;- model.matrix(~ 0 + treatments)\ncolnames(design) &lt;- levels\n\nfit &lt;- lmFit(yfilt, design = design)\ncontrast_pairs &lt;- expand.grid(x = levels, y = levels) %&gt;%\n    mutate(\n        flag = if_else(x != y, T, F),\n        pair = paste0(x, \"-\", y)\n    ) %&gt;%\n    filter(flag) %&gt;%\n    pull(pair) %&gt;%\n    unique()\ncontrast_matrix &lt;- makeContrasts(contrasts = contrast_pairs, levels = design)\nfit2 &lt;- contrasts.fit(fit, contrast_matrix)\nfit2 &lt;- eBayes(fit2, trend = T, robust = T)\n\nfor (pair in contrast_pairs) {\n    res &lt;- topTable(fit2, coef = pair, number = Inf, adjust.method = \"BH\", p.value = 1)\n    vroom_write(res, file = file.path(out_dir, paste0(pair, \".tsv\")), delim = \"\\t\")\n}"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#filter-degs-and-plot-volcanos",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#filter-degs-and-plot-volcanos",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "8 Filter DEGs and plot volcanos",
    "text": "8 Filter DEGs and plot volcanos\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\n\nset_fonts()\n\ndegs_dir &lt;- \"./data/degs\"\nout_dir &lt;- \"./data/clean_degs/padj0.05_logfc1\"\nsig_colors &lt;- c(\"#FF4757\", \"#546DE5\", \"#D2DAE2\")\npadj_th &lt;- 0.05\nlogfc_th &lt;- 1\n\ndir.create(out_dir, recursive = T)\n\ndegs_files &lt;- list.files(degs_dir, pattern = \"\\\\.tsv$\", full.names = T, recursive = F)\nfor (degs_file in degs_files) {\n    pair &lt;- strsplit(gsub(\"\\\\.[a-zA-Z0-9]+$\", \"\", basename(degs_file)),\n        split = \"-\", fixed = T\n    )[[1]]\n\n    degs_df &lt;- vroom(degs_file, delim = \"\\t\", col_names = T)\n    clean_degs_df &lt;- degs_df %&gt;%\n        group_by(Symbol) %&gt;%\n        slice_min(adj.P.Val, n = 1) %&gt;%\n        slice_max(abs(logFC), n = 1) %&gt;%\n        slice_sample(n = 1) %&gt;%\n        ungroup() %&gt;%\n        mutate(\n            diff_flag = if_else(adj.P.Val &lt; padj_th,\n                if_else(logFC &gt; logfc_th,\n                    paste0(pair[1], \" Up\"),\n                    if_else(logFC &lt; -logfc_th,\n                        paste0(pair[2], \" Up\"),\n                        \"NO\"\n                    )\n                ),\n                \"NO\"\n            ),\n            diff_flag = factor(diff_flag, levels = c(\n                paste0(pair[1], \" Up\"),\n                paste0(pair[2], \" Up\"),\n                \"NO\"\n            ))\n        )\n\n    vroom_write(clean_degs_df, file = file.path(out_dir, basename(degs_file)))\n\n    if (any(duplicated(clean_degs_df$Symbol))) {\n        stop(\"Duplicated items still existed after filtering for \", degs_file)\n    }\n\n    diff_counts &lt;- count(clean_degs_df, diff_flag) %&gt;%\n        mutate(show_text = paste0(diff_flag, \": \", n))\n    plot_title &lt;- paste0(\n        paste0(pair, collapse = \"_vs_\"), \"\\n\",\n        paste0(diff_counts$show_text, collapse = \"    \")\n    )\n\n    p &lt;- ggplot(clean_degs_df) +\n        geom_point(aes(logFC, -log10(adj.P.Val), color = diff_flag),\n            alpha = 0.5, size = 2\n        ) +\n        geom_vline(\n            xintercept = c(-logfc_th, logfc_th),\n            linewidth = 1, col = \"grey25\", linetype = \"dashed\"\n        ) +\n        geom_hline(\n            yintercept = -log10(padj_th),\n            linewidth = 1, col = \"grey25\", linetype = \"dashed\"\n        ) +\n        scale_color_manual(values = setNames(sig_colors, levels(clean_degs_df$diff_flag))) +\n        labs(\n            title = plot_title,\n            x = \"log2 Fold Change\", y = \"-log10(p.adjust)\",\n            color = paste0(\"p.adjust &lt; \", padj_th, \"\\n|log2(FC)| &gt; \", logfc_th)\n        ) +\n        theme_classic() +\n        theme(\n            plot.title = element_text(hjust = 0.5),\n            axis.title.x = element_text(size = 26),\n            axis.title.y = element_text(size = 26),\n            axis.text.x = element_text(size = 24),\n            axis.text.y = element_text(size = 24),\n            legend.text = element_text(size = 24),\n            legend.title = element_text(size = 26),\n            text = element_text(family = \"Arial\")\n        )\n\n    ppreview(p, file = file.path(\n        out_dir,\n        gsub(\"\\\\.[a-zA-Z0-9]+$\", \".pdf\", basename(degs_file))\n    ))\n}"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#go-analysis",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#go-analysis",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "9 GO analysis",
    "text": "9 GO analysis\n\n9.1 Prepare ferret gene IDs\n\nlibrary(vroom)\nlibrary(tidyverse)\n\ngpl_file &lt;- \"./data/microarray/gpl.with_gene_symbols.tsv\"\nout_dir &lt;- \"./data/go\"\n\ndir.create(out_dir)\n\ngpl &lt;- vroom(gpl_file, delim = \"\\t\")\ngpl$ProbeName &lt;- sapply(gpl$qseqid, function(x) {\n    strsplit(x, \":\")[[1]][4]\n})\ngpl &lt;- gpl %&gt;%\n    select(all_of(c(\n        \"ProbeName\", \"Dbxref_GeneID\", \"gene\",\n        \"gene_biotype.GPL_blastn\", \"gene_biotype.Parent_gene\"\n    ))) %&gt;%\n    mutate(GeneBioType = if_else(is.na(gene_biotype.Parent_gene),\n        gene_biotype.GPL_blastn,\n        gene_biotype.Parent_gene\n    )) %&gt;%\n    select(all_of(c(\"ProbeName\", \"Dbxref_GeneID\", \"gene\", \"GeneBioType\"))) %&gt;%\n    distinct()\nnames(gpl) &lt;- c(\"ProbeName\", \"EntrezID\", \"Symbol\", \"GeneBioType\")\n\nvroom_write_lines(as.character(sort(unique(gpl$EntrezID))),\n    file = file.path(out_dir, \"ferret_entrezids.txt\")\n)\n\n\n\n9.2 Prepare orthologous gene set between mm10 and ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\n\nferret_mm_orthologs_file &lt;- \"./data/go/ferret_mm10.orthologs.tsv\"\nferret_dataset_file &lt;- \"./data/go/ferret_ncbi_dataset.tsv\"\n\nferret_dataset_df &lt;- vroom(ferret_dataset_file) %&gt;%\n    select(all_of(c(\"NCBI GeneID\", \"Symbol\", \"Gene Type\", \"Gene Group Identifier\"))) %&gt;%\n    set_colnames(c(\"ferret_EntrezID\", \"ferret_Symbol\", \"ferret_GeneType\", \"Ortholog_Group_Identifier\")) %&gt;%\n    na.omit() %&gt;%\n    mutate_all(as.character) %&gt;%\n    distinct()\nferret_mm_orthologs_df &lt;- vroom(ferret_mm_orthologs_file) %&gt;%\n    select(all_of(c(\"NCBI GeneID\", \"Symbol\", \"Gene Group Identifier\"))) %&gt;%\n    set_colnames(c(\"mm10_EntrezID\", \"mm10_Symbol\", \"Ortholog_Group_Identifier\")) %&gt;%\n    na.omit() %&gt;%\n    mutate_all(as.character) %&gt;%\n    distinct()\ndf &lt;- inner_join(ferret_dataset_df, ferret_mm_orthologs_df, by = \"Ortholog_Group_Identifier\")\nvroom_write(df, file = file.path(dirname(ferret_mm_orthologs_file), \"ferret_mm10.orthologs.clean.tsv\"))\n\n\n\n9.3 Attach orthologous mm10 gene IDs to DEGs of ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nferret_mm_orthologs_file &lt;- \"./data/go/ferret_mm10.orthologs.clean.tsv\"\ndegs_files &lt;- c(\n    \"./data/clean_degs/padj0.05_logfc1/A17.VZ-A19.VZ.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.ISVZ-A19.ISVZ.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.OSVZ-A19.OSVZ.tsv\"\n)\n\nferret_mm_orthologs_df &lt;- vroom(ferret_mm_orthologs_file) %&gt;%\n    mutate_all(as.character)\nfor (degs_file in degs_files) {\n    vroom(degs_file) %&gt;%\n        select(all_of(c(\n            \"EntrezID\", \"Symbol\",\n            \"logFC\", \"P.Value\", \"adj.P.Val\", \"diff_flag\"\n        ))) %&gt;%\n        mutate(EntrezID = as.character(EntrezID)) %&gt;%\n        inner_join(ferret_mm_orthologs_df, by = c(\"EntrezID\" = \"ferret_EntrezID\")) %&gt;%\n        arrange(desc(logFC)) %&gt;%\n        distinct() %&gt;%\n        filter(ferret_GeneType == \"PROTEIN_CODING\") %&gt;%\n        vroom_write(file = gsub(\"tsv$\", \"with_mm10_IDs.tsv\", degs_file))\n}"
  },
  {
    "objectID": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#check-the-differential-expression-concordance-between-samples-of-mink-and-samples-of-ferret",
    "href": "posts/bulk RNA-seq/agilent_mrna_expression_microarray_analysis_using_limma/index.html#check-the-differential-expression-concordance-between-samples-of-mink-and-samples-of-ferret",
    "title": "Agilent mRNA expression microarray analysis using limma",
    "section": "10 Check the differential expression concordance between samples of mink and samples of ferret",
    "text": "10 Check the differential expression concordance between samples of mink and samples of ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nmink_degs_file &lt;- \"H:/ubuntu_ssd_backup/projects/mink/proj/rna/degs/res/only_degs/mm10/P2_Gyr_vs_P2_Sul.txt\"\ndegs_files &lt;- c(\n    \"./data/clean_degs/padj0.05_logfc1/A17.VZ-A19.VZ.with_mm10_IDs.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.ISVZ-A19.ISVZ.with_mm10_IDs.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.OSVZ-A19.OSVZ.with_mm10_IDs.tsv\"\n)\n\nmink_degs &lt;- vroom(mink_degs_file) %&gt;%\n    select(all_of(c(\"final_gene_name\", \"diff_flag\"))) %&gt;%\n    distinct()\ndegs_df &lt;- tibble()\nfor (degs_file in degs_files) {\n    tmp_df &lt;- vroom(degs_file) %&gt;%\n        select(all_of(c(\"mm10_Symbol\", \"logFC\"))) %&gt;%\n        mutate(tissue = gsub(\n            \"A17\\\\.[A-Z]+-A19\\\\.|\\\\.with_mm10_IDs\\\\.tsv$\",\n            \"\", basename(degs_file)\n        )) %&gt;%\n        distinct()\n    degs_df &lt;- bind_rows(degs_df, tmp_df)\n}\ndf &lt;- inner_join(mink_degs, degs_df, by = c(\"final_gene_name\" = \"mm10_Symbol\")) %&gt;%\n    mutate(\n        tissue = factor(tissue, levels = c(\"VZ\", \"ISVZ\", \"OSVZ\")),\n        logFC_sign = if_else(logFC &gt;= 0, \"+\", \"-\")\n    )\n\nggplot(df) +\n    geom_jitter(aes(tissue, logFC, color = diff_flag))"
  }
]